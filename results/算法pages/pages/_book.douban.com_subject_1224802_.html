

<!DOCTYPE html>
<html lang="zh-cmn-Hans" class="ua-windows ua-ie6 book-new-nav">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <title>统计自然语言处理基础 (豆瓣)</title>
  
<script>!function(e){var o=function(o,n,t){var c,i,r=new Date;n=n||30,t=t||"/",r.setTime(r.getTime()+24*n*60*60*1e3),c="; expires="+r.toGMTString();for(i in o)e.cookie=i+"="+o[i]+c+"; path="+t},n=function(o){var n,t,c,i=o+"=",r=e.cookie.split(";");for(t=0,c=r.length;t<c;t++)if(n=r[t].replace(/^\s+|\s+$/g,""),0==n.indexOf(i))return n.substring(i.length,n.length).replace(/\"/g,"");return null},t=e.write,c={"douban.com":1,"douban.fm":1,"google.com":1,"google.cn":1,"googleapis.com":1,"gmaptiles.co.kr":1,"gstatic.com":1,"gstatic.cn":1,"google-analytics.com":1,"googleadservices.com":1},i=function(e,o){var n=new Image;n.onload=function(){},n.src="https://www.douban.com/j/except_report?kind=ra022&reason="+encodeURIComponent(e)+"&environment="+encodeURIComponent(o)},r=function(o){try{t.call(e,o)}catch(e){t(o)}},a=/<script.*?src\=["']?([^"'\s>]+)/gi,g=/http:\/\/(.+?)\.([^\/]+).+/i;e.writeln=e.write=function(e){var t,l=a.exec(e);return l&&(t=g.exec(l[1]))?c[t[2]]?void r(e):void("tqs"!==n("hj")&&(i(l[1],location.href),o({hj:"tqs"},1),setTimeout(function(){location.replace(location.href)},50))):void r(e)}}(document);
</script>

  
  <meta http-equiv="Pragma" content="no-cache">
  <meta http-equiv="Expires" content="Sun, 6 Mar 2005 01:00:00 GMT">
  
<meta http-equiv="mobile-agent" content="format=html5; url=https://m.douban.com/book/subject/1224802/">
<meta name="keywords" content="统计自然语言处理基础,Chris Manning,Hinrich Schütze,电子工业出版社,2005-1,简介,作者,书评,论坛,推荐,二手">
<meta name="description" content="图书统计自然语言处理基础 介绍、书评、论坛及推荐 ">

  <script>var _head_start = new Date();</script>
  
  <link href="https://img3.doubanio.com/f/book/c562010083631a8b5625ac07bc2a6e7e4466b6f6/css/book/master.css" rel="stylesheet" type="text/css">

  <link href="https://img3.doubanio.com/f/book/8a9d097c416aabac4f7e45f5d2ce9c6834b3fc7a/css/book/base/init.css" rel="stylesheet">
  <style type="text/css"></style>
  <script src="https://img3.doubanio.com/f/book/0495cb173e298c28593766009c7b0a953246c5b5/js/book/lib/jquery/jquery.js"></script>
  <script src="https://img3.doubanio.com/f/book/0f1957d4c436280f238b0295e4d0ba6855510555/js/book/master.js"></script>
  

  
  <link rel="stylesheet" href="https://img3.doubanio.com/f/book/15aa82bcdf414e007dee7483304105800fc59e54/css/book/subject.css">
  <link href="https://img3.doubanio.com/f/book/5d301503fbbd8e09f3114583859789884e942f47/css/book/annotation/like.css" rel="stylesheet">
  <script src="https://img3.doubanio.com/f/shire/3c6f2946669cfb2fc9ee4a4d1dcc41fc181cad92/js/lib/jquery.snippet.js"></script>
  <script src="https://img3.doubanio.com/f/shire/77323ae72a612bba8b65f845491513ff3329b1bb/js/do.js" data-cfg-autoload="false"></script>
  <script src="https://img3.doubanio.com/f/shire/4ea3216519a6183c7bcd4f7d1a6d4fd57ce1a244/js/ui/dialog.js"></script>
  <script src="https://img3.doubanio.com/f/book/2e421e5ec8f2869d31535206c0ac0322532be1f8/js/book/mod/hide.js"></script>
  <script src="https://img3.doubanio.com/f/book/7196bdec780f03785f55b06fda34999595057f65/js/book/subject/unfold.js"></script>
    <link rel="alternate" href="https://book.douban.com/feed/subject/1224802/reviews" type="application/rss+xml" title="RSS">
  <style type="text/css"> h2 {color: #007722;} </style>
  <script type='text/javascript'>
    var _vds = _vds || [];
    (function(){ _vds.push(['setAccountId', '22c937bbd8ebd703f2d8e9445f7dfd03']);
        _vds.push(['setCS1','user_id','0']);
            (function() {var vds = document.createElement('script');
                vds.type='text/javascript';
                vds.async = true;
                vds.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'dn-growing.qbox.me/vds.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(vds, s);
            })();
    })();
</script>

  
  <script type='text/javascript'>
    var _vwo_code=(function(){
      var account_id=249272,
          settings_tolerance=2000,
          library_tolerance=2500,
          use_existing_jquery=false,
          // DO NOT EDIT BELOW THIS LINE
          f=false,d=document;return{use_existing_jquery:function(){return use_existing_jquery;},library_tolerance:function(){return library_tolerance;},finish:function(){if(!f){f=true;var a=d.getElementById('_vis_opt_path_hides');if(a)a.parentNode.removeChild(a);}},finished:function(){return f;},load:function(a){var b=d.createElement('script');b.src=a;b.type='text/javascript';b.innerText;b.onerror=function(){_vwo_code.finish();};d.getElementsByTagName('head')[0].appendChild(b);},init:function(){settings_timer=setTimeout('_vwo_code.finish()',settings_tolerance);var a=d.createElement('style'),b='body{opacity:0 !important;filter:alpha(opacity=0) !important;background:none !important;}',h=d.getElementsByTagName('head')[0];a.setAttribute('id','_vis_opt_path_hides');a.setAttribute('type','text/css');if(a.styleSheet)a.styleSheet.cssText=b;else a.appendChild(d.createTextNode(b));h.appendChild(a);this.load('//dev.visualwebsiteoptimizer.com/j.php?a='+account_id+'&u='+encodeURIComponent(d.URL)+'&r='+Math.random());return settings_timer;}};}());_vwo_settings_timer=_vwo_code.init();
  </script>


  <script>  </script>
  <style type="text/css">
.tag,.tag:link,.tag:visited{width:auto;word-break:keep-all;white-space:nowrap;background-color:#f5f5f5;color:#37A;font-size:13px;padding:2px 11px 0;display:inline-block;margin:0 3px 5px 0;line-height:20px}.tag:hover,.tag:active{background-color:#e8e8e8;color:#37A}


#db-discussion-section .olt { margin-bottom: 7px; }
</style>

  <link rel="shortcut icon" href="https://img3.doubanio.com/favicon.ico" type="image/x-icon">
</head>
<body>
  
    <script>var _body_start = new Date();</script>
    
  



    <link href="//img3.doubanio.com/dae/accounts/resources/321e246/shire/bundle.css" rel="stylesheet" type="text/css">



<div id="db-global-nav" class="global-nav">
  <div class="bd">
    
<div class="top-nav-info">
  <a href="https://www.douban.com/accounts/login?source=book" class="nav-login" rel="nofollow">登录</a>
  <a href="https://www.douban.com/accounts/register?source=book" class="nav-register" rel="nofollow">注册</a>
</div>


    
<div class="top-nav-doubanapp">
  <a href="https://www.douban.com/doubanapp/app?channel=top-nav" class="lnk-doubanapp">下载豆瓣客户端</a>
  <div id="top-nav-appintro" class="more-items">
    <p class="appintro-title">豆瓣</p>
    <p class="slogan">我们的精神角落</p>
    <p class="qrcode">扫码直接下载</p>
    <div class="download">
      <a href="https://www.douban.com/doubanapp/redirect?channel=top-nav&direct_dl=1&download=iOS">iPhone</a>
      <span>·</span>
      <a href="https://www.douban.com/doubanapp/redirect?channel=top-nav&direct_dl=1&download=Android" class="download-android">Android</a>
    </div>
    <div id="doubanapp-tip">
      <a href="https://www.douban.com/doubanapp/app?channel=qipao" class="tip-link">豆瓣 5.0 全新发布</a>
      <a href="javascript: void 0;" class="tip-close">×</a>
    </div>
  </div>
</div>

    


<div class="global-nav-items">
  <ul>
    <li class="">
      <a href="https://www.douban.com" target="_blank" data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-main&quot;,&quot;uid&quot;:&quot;0&quot;}">豆瓣</a>
    </li>
    <li class="on">
      <a href="https://book.douban.com"  data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-book&quot;,&quot;uid&quot;:&quot;0&quot;}">读书</a>
    </li>
    <li class="">
      <a href="https://movie.douban.com" target="_blank" data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-movie&quot;,&quot;uid&quot;:&quot;0&quot;}">电影</a>
    </li>
    <li class="">
      <a href="https://music.douban.com" target="_blank" data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-music&quot;,&quot;uid&quot;:&quot;0&quot;}">音乐</a>
    </li>
    <li class="">
      <a href="https://www.douban.com/location" target="_blank" data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-location&quot;,&quot;uid&quot;:&quot;0&quot;}">同城</a>
    </li>
    <li class="">
      <a href="https://www.douban.com/group" target="_blank" data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-group&quot;,&quot;uid&quot;:&quot;0&quot;}">小组</a>
    </li>
    <li class="">
      <a href="https://read.douban.com&#47;?dcs=top-nav&amp;dcm=douban" target="_blank" data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-read&quot;,&quot;uid&quot;:&quot;0&quot;}">阅读</a>
    </li>
    <li class="">
      <a href="https://douban.fm&#47;?from_=shire_top_nav" target="_blank" data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-fm&quot;,&quot;uid&quot;:&quot;0&quot;}">FM</a>
    </li>
    <li class="">
      <a href="https://time.douban.com&#47;?dt_time_source=douban-web_top_nav" target="_blank" data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-time&quot;,&quot;uid&quot;:&quot;0&quot;}">时间</a>
    </li>
    <li class="">
      <a href="https://market.douban.com&#47;?utm_campaign=douban_top_nav&amp;utm_source=douban&amp;utm_medium=pc_web" target="_blank" data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-market&quot;,&quot;uid&quot;:&quot;0&quot;}">市集</a>
    </li>
    <li>
      <a href="#more" class="bn-more"><span>更多</span></a>
      <div class="more-items">
        <table cellpadding="0" cellspacing="0">
          <tbody>
            <tr>
              <td>
                <a href="https://ypy.douban.com" target="_blank" data-moreurl-dict="{&quot;from&quot;:&quot;top-nav-click-ypy&quot;,&quot;uid&quot;:&quot;0&quot;}">豆瓣摄影</a>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
    </li>
  </ul>
</div>

  </div>
</div>
<script>
  ;window._GLOBAL_NAV = {
    DOUBAN_URL: "https://www.douban.com",
    N_NEW_NOTIS: 0,
    N_NEW_DOUMAIL: 0
  };
</script>



    <script src="//img3.doubanio.com/dae/accounts/resources/321e246/shire/bundle.js" defer="defer"></script>




  



    <link href="//img3.doubanio.com/dae/accounts/resources/094108a/book/bundle.css" rel="stylesheet" type="text/css">




<div id="db-nav-book" class="nav">
  <div class="nav-wrap">
  <div class="nav-primary">
    <div class="nav-logo">
      <a href="https:&#47;&#47;book.douban.com">豆瓣读书</a>
    </div>
    <div class="nav-search">
      <form action="https:&#47;&#47;book.douban.com/subject_search" method="get">
        <fieldset>
          <legend>搜索：</legend>
          <label for="inp-query">
          </label>
          <div class="inp"><input id="inp-query" name="search_text" size="22" maxlength="60" placeholder="书名、作者、ISBN" value=""></div>
          <div class="inp-btn"><input type="submit" value="搜索"></div>
          <input type="hidden" name="cat" value="1001" />
        </fieldset>
      </form>
    </div>
  </div>
  </div>
  <div class="nav-secondary">
    

<div class="nav-items">
  <ul>
    <li    ><a href="https://book.douban.com/cart/"
     >购书单</a>
    </li>
    <li    ><a href="https://read.douban.com/ebooks/?dcs=book-nav&dcm=douban"
            target="_blank"
     >电子图书</a>
    </li>
    <li    ><a href="https://market.douban.com/book?utm_campaign=book_nav_freyr&utm_source=douban&utm_medium=pc_web"
     >豆瓣书店</a>
    </li>
    <li    ><a href="https://book.douban.com/annual/2017?source=navigation"
            target="_blank"
     >2017年度榜单</a>
    </li>
    <li    ><a href="https://book.douban.com/standbyme/2016?source=navigation"
            target="_blank"
     >2016读书报告</a>
    </li>
    <li          class=" book-cart"
    ><a href="https://market.douban.com/cart/?utm_campaign=book_nav_cart&utm_source=douban&utm_medium=pc_web"
            target="_blank"
     >购物车</a>
    </li>
  </ul>
</div>

  </div>
    <a href="https://book.douban.com/annual/2017?source=patch" class="bookannual2017"></a>
</div>

<script id="suggResult" type="text/x-jquery-tmpl">
  <li data-link="{{= url}}">
            <a href="{{= url}}" onclick="moreurl(this, {from:'book_search_sugg', query:'{{= keyword }}', subject_id:'{{= id}}', i: '{{= index}}', type: '{{= type}}'})">
            <img src="{{= pic}}" width="40" />
            <div>
                <em>{{= title}}</em>
                {{if year}}
                    <span>{{= year}}</span>
                {{/if}}
                <p>
                {{if type == "b"}}
                    {{= author_name}}
                {{else type == "a" }}
                    {{if en_name}}
                        {{= en_name}}
                    {{/if}}
                {{/if}}
                 </p>
            </div>
        </a>
        </li>
  </script>




    <script src="//img3.doubanio.com/dae/accounts/resources/094108a/book/bundle.js" defer="defer"></script>





    <div id="wrapper">
        
    <div id="dale_book_subject_top_icon"></div>
<h1>
    <span property="v:itemreviewed">统计自然语言处理基础</span>
    <div class="clear"></div>
</h1>

        
  <div id="content">
    
    <div class="grid-16-8 clearfix">
      
      <div class="article">



<div class="indent">
  <div class="subjectwrap clearfix">
    



<div class="subject clearfix">
<div id="mainpic" class="">

  

  <a class="nbg"
      href="https://img3.doubanio.com/lpic/s1298882.jpg" title="统计自然语言处理基础">
    <img src="https://img3.doubanio.com/lpic/s1298882.jpg" title="点击看大图" alt="统计自然语言处理基础"
         rel="v:photo" style="width: 135px">
  </a>



</div>





<div id="info" class="">


    
    
  
    <span>
      <span class="pl"> 作者</span>:
        
        
        <a class="" href="/search/Chris%20Manning">Chris Manning</a>
        
           /
        
        <a class="" href="/search/Hinrich%20Sch%C3%BCtze">Hinrich Schütze</a>
    </span><br/>

    
    
  
    <span class="pl">出版社:</span> 电子工业出版社<br/>

    
    
  

    
    
  

    
    
  
    <span class="pl">原作名:</span> Foundations of Statistical Natural Language Processing<br/>

    
    
  
    <span>
      <span class="pl"> 译者</span>:
        
        
        <a class="" href="/search/%E8%8B%91%E6%98%A5%E6%B3%95">苑春法</a>
        
           /
        
        <a class="" href="/search/%E6%9D%8E%E4%BC%9F">李伟</a>
        
           /
        
        <a class="" href="/search/%E6%9D%8E%E5%BA%86%E4%B8%AD">李庆中</a>
    </span><br/>

    
    
  
    <span class="pl">出版年:</span> 2005-1<br/>

    
    
  
    <span class="pl">页数:</span> 418<br/>

    
    
  
    <span class="pl">定价:</span> 55.00元<br/>

    
    
  
    <span class="pl">装帧:</span> 平装(无盘)<br/>

    
    
  
    <span class="pl">丛书:</span>&nbsp;<a href="https://book.douban.com/series/35164">国外计算机科学教材系列</a><br>

    
    
  
    
      
      <span class="pl">ISBN:</span> 9787505399211<br/>


</div>

</div>





















    





<div id="interest_sectl" class="">
  <div class="rating_wrap clearbox" rel="v:rating">
    <div class="rating_logo">豆瓣评分</div>
    <div class="rating_self clearfix" typeof="v:Rating">
      <strong class="ll rating_num " property="v:average"> 8.1 </strong>
      <span property="v:best" content="10.0"></span>
      <div class="rating_right ">
          <div class="ll bigstar40"></div>
            <div class="rating_sum">
                <span class="">
                    <a href="collections" class="rating_people"><span property="v:votes">138</span>人评价</a>
                </span>
            </div>


      </div>
    </div>
          
            
            
<span class="stars5 starstop" title="力荐">
    5星
</span>

            
<div class="power" style="width:51px"></div>

            <span class="rating_per">36.2%</span>
            <br>
            
            
<span class="stars4 starstop" title="推荐">
    4星
</span>

            
<div class="power" style="width:64px"></div>

            <span class="rating_per">44.9%</span>
            <br>
            
            
<span class="stars3 starstop" title="还行">
    3星
</span>

            
<div class="power" style="width:22px"></div>

            <span class="rating_per">15.9%</span>
            <br>
            
            
<span class="stars2 starstop" title="较差">
    2星
</span>

            
<div class="power" style="width:2px"></div>

            <span class="rating_per">1.4%</span>
            <br>
            
            
<span class="stars1 starstop" title="很差">
    1星
</span>

            
<div class="power" style="width:2px"></div>

            <span class="rating_per">1.4%</span>
            <br>
    </div>
</div>

  </div>
  





  
    
    <div id="interest_sect_level" class="clearfix">
        <a href="#" rel="nofollow" class="j a_show_login colbutt ll" name="pbtn-1224802-wish">
          <span>
            
<form method="POST" action="https://www.douban.com/register?reason=collectwish" class="miniform">
    <input type="submit" class="minisubmit j " value="想读" title="" />
</form>

          </span>
        </a>
        <a href="#" rel="nofollow" class="j a_show_login colbutt ll" name="pbtn-1224802-do">
          <span>
            
<form method="POST" action="https://www.douban.com/register?reason=collectdo" class="miniform">
    <input type="submit" class="minisubmit j " value="在读" title="" />
</form>

          </span>
        </a>
        <a href="#" rel="nofollow" class="j a_show_login colbutt ll" name="pbtn-1224802-collect">
          <span>
            
<form method="POST" action="https://www.douban.com/register?reason=collectcollect" class="miniform">
    <input type="submit" class="minisubmit j " value="读过" title="" />
</form>

          </span>
        </a>
      <div class="ll j a_stars">
        
    
    评价:
    <span id="rating"> <span id="stars" data-solid="https://img3.doubanio.com/f/shire/5a2327c04c0c231bced131ddf3f4467eb80c1c86/pics/rating_icons/star_onmouseover.png" data-hollow="https://img3.doubanio.com/f/shire/2520c01967207a1735171056ec588c8c1257e5f8/pics/rating_icons/star_hollow_hover.png" data-solid-2x="https://img3.doubanio.com/f/shire/7258904022439076d57303c3b06ad195bf1dc41a/pics/rating_icons/star_onmouseover@2x.png" data-hollow-2x="https://img3.doubanio.com/f/shire/95cc2fa733221bb8edd28ad56a7145a5ad33383e/pics/rating_icons/star_hollow_hover@2x.png">

            <a href="https://www.douban.com/register?reason=rate" class="j a_show_login" name="pbtn-1224802-1">
        <img src="https://img3.doubanio.com/f/shire/2520c01967207a1735171056ec588c8c1257e5f8/pics/rating_icons/star_hollow_hover.png" id="star1" width="16" height="16"/></a>
            <a href="https://www.douban.com/register?reason=rate" class="j a_show_login" name="pbtn-1224802-2">
        <img src="https://img3.doubanio.com/f/shire/2520c01967207a1735171056ec588c8c1257e5f8/pics/rating_icons/star_hollow_hover.png" id="star2" width="16" height="16"/></a>
            <a href="https://www.douban.com/register?reason=rate" class="j a_show_login" name="pbtn-1224802-3">
        <img src="https://img3.doubanio.com/f/shire/2520c01967207a1735171056ec588c8c1257e5f8/pics/rating_icons/star_hollow_hover.png" id="star3" width="16" height="16"/></a>
            <a href="https://www.douban.com/register?reason=rate" class="j a_show_login" name="pbtn-1224802-4">
        <img src="https://img3.doubanio.com/f/shire/2520c01967207a1735171056ec588c8c1257e5f8/pics/rating_icons/star_hollow_hover.png" id="star4" width="16" height="16"/></a>
            <a href="https://www.douban.com/register?reason=rate" class="j a_show_login" name="pbtn-1224802-5">
        <img src="https://img3.doubanio.com/f/shire/2520c01967207a1735171056ec588c8c1257e5f8/pics/rating_icons/star_hollow_hover.png" id="star5" width="16" height="16"/></a>
    </span><span id="rateword" class="pl"></span>
    <input id="n_rating" type="hidden" value=""  />
    </span>

      </div>
      

    </div>



  
  <div class="gtleft">
    <ul class="ul_subject_menu bicelink color_gray pt6 clearfix">
        <li>
          <img src="https://img3.doubanio.com/f/shire/5bbf02b7b5ec12b23e214a580b6f9e481108488c/pics/add-review.gif" />&nbsp;<a href="https://www.douban.com/register?reason=annotate" class="j a_show_login" rel="nofollow">写笔记</a>
        </li>

          <li>
            <img src="https://img3.doubanio.com/f/shire/5bbf02b7b5ec12b23e214a580b6f9e481108488c/pics/add-review.gif" />&nbsp;<a class="j a_show_login" href="https://www.douban.com/register?reason=review" rel="nofollow">写书评</a>
          </li>

      <li>

  <span class="rr">
  

    <img src="https://img3.doubanio.com/pics/add-cart.gif"/>
      <a class="j a_show_login" href="http://http://www.douban.com/register?reason=addbook2cart" rel="nofollow">加入购书单</a>
  <span class="hidden">已在<a href="https://book.douban.com/cart">购书单</a></span>
</span><br class="clearfix" />
</li>


        
        
    
    <li class="rec" id="C-1224802">
        <a href="#" data-url="https://book.douban.com/subject/1224802/" data-desc="" data-title="书籍《统计自然语言处理基础》 (来自豆瓣) " data-pic="https://img3.doubanio.com/lpic/s1298882.jpg" class="bn-sharing ">分享到</a> &nbsp;&nbsp;
    </li>
    <script>
    var __cache_url = __cache_url || {};
    (function(u){
        if(__cache_url[u]) return;
        __cache_url[u] = true;
        window.DoubanShareIcons = 'https://img3.doubanio.com/f/shire/d15ffd71f3f10a7210448fec5a68eaec66e7f7d0/pics/ic_shares.png';
        var initShareButton = function() {
          $.ajax({url:u,dataType:'script',cache:true});
        };
        if (typeof Do == 'function' && 'ready' in Do) {
          Do('https://img3.doubanio.com/f/shire/8377b9498330a2e6f056d863987cc7a37eb4d486/css/ui/dialog.css',
            'https://img3.doubanio.com/f/shire/4ea3216519a6183c7bcd4f7d1a6d4fd57ce1a244/js/ui/dialog.js',
            initShareButton);
        } else if(typeof Douban == 'object' && 'loader' in Douban) {
          Douban.loader.batch(
            'https://img3.doubanio.com/f/shire/8377b9498330a2e6f056d863987cc7a37eb4d486/css/ui/dialog.css',
            'https://img3.doubanio.com/f/shire/4ea3216519a6183c7bcd4f7d1a6d4fd57ce1a244/js/ui/dialog.js'
          ).done(initShareButton);
        }
    })('https://img3.doubanio.com/f/shire/6e6a5f21daeec19bbb41bf48c07fccaa4dad4d98/js/lib/sharebutton.js');
    </script>

    </ul>
  </div>


    







<div class="rec-sec">

    <span class="rec">

<a href="https://www.douban.com/accounts/register?reason=collect" class="j a_show_login lnk-sharing lnk-douban-sharing">推荐</a>
</span>
</div>


<script>
  //bind events for collection button.
  $('.collect_btn', '#interest_sect_level').each(function(){
      Douban.init_collect_btn(this);
  });
</script>








</div>

<br clear="all">
<div id="collect_form_1224802"></div>
<div class="related_info">
  






  

  <h2>
    <span class="">内容简介</span>
      ······

  </h2>



<div class="indent" id="link-report">
    
      <div class="">
        <style type="text/css" media="screen">
.intro p{text-indent:2em;word-break:normal;}
</style>
<div class="intro">
    <p>《统计自然语言处理基础：国外计算机科学教材系列》是一本全面系统地介绍统计自然语言处理技术的专著，被国内外许多所著名大学选为计算语言学相关课程的教材。《统计自然语言处理基础：国外计算机科学教材系列》涵盖的内容十分广泛，分为四个部分，共16章，包括了构建自然语言处理软件工具将用到的几乎所有理论和算法。全书的论述过程由浅入深，从数学基础到精确的理论算法，从简单的词法分析到复杂的语法分析，适合不同水平的读者群的需求。同时，《统计自然语言处理基础：国外计算机科学教材系列》将理论与实践紧密联系在一起，在介绍理论知识的基础上给出了自然语言处理技术的高层应用（如信息检索等）。在《统计自然语言处理基础：国外计算机科学教材系列》的配套网站上提供了许多相关资源和工具，便于读者结合书中习题，在实践中获得提高。近年来，自然语言处理中的统计学方法已经逐渐成为主流。</p></div>

      </div>
    
<style>
    #link-report .report { text-align: right; font-size: 12px; visibility: hidden; }
    #link-report .report a { color: #BBB; }
    #link-report .report a:hover { color: #FFF; background-color: #BBB; }
</style>
<script>
    Do = (typeof Do === 'undefined')? $ : Do;
    Do(function(){
    $("body").delegate("#link-report", 'mouseenter mouseleave', function(e){
      switch (e.type) {
        case "mouseenter":
          $(this).find(".report").css('visibility', 'visible');
          break;
        case "mouseleave":
          $(this).find(".report").css('visibility', 'hidden');
          break;
      }
    });
    $("#link-report").delegate(".report a", 'click', function(e){
        e.preventDefault();
        var auditUrl = "https://www.douban.com/misc/audit_report?url=",
        opt = "";
        var obj = $(e.target).closest('#link-report');
        var id = obj.length != 0 ? obj.data("id") : undefined;
        var params = (opt&&id) ? '?'.concat(opt, '=', id) : '';
        var url = auditUrl.concat("https://book.douban.com/subject/1224802/", params);
        window.location.href = url;
    });

    $("#link-report").append('<div class="report"><a rel="nofollow" href="#">举报</a></div>');
  });
</script>

</div>

  



































































  

  
    




  

  <h2>
    <span class="">目录</span>
      ······

  </h2>



<div class="indent" id="dir_1224802_short">
        第一部分 基础知识<br/>
        第一章 绪论<br/>
        第二章 数学基础<br/>
        第三章 语言学基础<br/>
        第四章 基于语料库的工作<br/>
        第二部分 词法<br/>
    · · · · · ·
    (<a href="javascript:$('#dir_1224802_short').hide();$('#dir_1224802_full').show();$.get('/j/subject/j_dir_count',{id:1224802});void(0);">更多</a>)
</div>

<div class="indent" id="dir_1224802_full" style="display:none">
        第一部分 基础知识<br/>
        第一章 绪论<br/>
        第二章 数学基础<br/>
        第三章 语言学基础<br/>
        第四章 基于语料库的工作<br/>
        第二部分 词法<br/>
        第五章 搭配<br/>
        第六章 统计推理：稀疏数据集上的n元语法模型<br/>
        第七章 语义消歧<br/>
        第八章 词汇获取<br/>
        第三部分 语法<br/>
        第九章 马尔可夫模型<br/>
        第十章 词性标注<br/>
        第十一章 概率上下文无关文法<br/>
        第十二章 概率句法分析<br/>
        第四部分 应用与技术<br/>
        第十三章 统计对齐和机器翻译<br/>
        第十四章 聚类<br/>
        第十五章 信息检索<br/>
        第十六章 文本分类<br/>
     · · · · · ·     (<a href="javascript:$('#dir_1224802_full').hide();$('#dir_1224802_short').show();void(0);">收起</a>)
</div>



  



<div id="author-wrapper" class="author-wrapper">
  <div class="loading"></div>
</div>
<link rel="stylesheet" href="https://img3.doubanio.com/f/fanta/ba954f353fb7e2f830059e78c8e9e4791a96a4f6/components/dist/css/answer_entry.css" />

<script type="text/javascript">
  var answerObj = {
    TYPE: 'book',
    SUBJECT_ID: '1224802',
    ISALL: 'False' || false,
    USER_ID: 'None'
  }
</script>
<script src="https://img3.doubanio.com/f/book/61252f2f9b35f08b37f69d17dfe48310dd295347/js/book/lib/react/bundle.js"></script>
<script src="https://img3.doubanio.com/f/fanta/f86ac0ee5349a9eeff2e7a3741853e8fc76d3042/components/dist/answer_entry.js"></script>

  






<div id="db-tags-section" class="blank20">
  
  

  <h2>
    <span class="">豆瓣成员常用的标签(共83个)</span>
      ······

  </h2>


  <div class="indent">    <span class="">
        <a class="  tag" href="/tag/自然语言处理">自然语言处理</a> &nbsp;    </span>
    <span class="">
        <a class="  tag" href="/tag/NLP">NLP</a> &nbsp;    </span>
    <span class="">
        <a class="  tag" href="/tag/计算语言学">计算语言学</a> &nbsp;    </span>
    <span class="">
        <a class="  tag" href="/tag/计算机">计算机</a> &nbsp;    </span>
    <span class="">
        <a class="  tag" href="/tag/统计">统计</a> &nbsp;    </span>
    <span class="">
        <a class="  tag" href="/tag/人工智能">人工智能</a> &nbsp;    </span>
    <span class="">
        <a class="  tag" href="/tag/自然语言">自然语言</a> &nbsp;    </span>
    <span class="">
        <a class="  tag" href="/tag/语言学">语言学</a> &nbsp;    </span>
  </div>
</div>


  


<div class="subject_show block5">
<h2>丛书信息</h2>
<div>
　　<a href="https://book.douban.com/series/35164">国外计算机科学教材系列 (共206册)</a>,
这套丛书还有
《人工智能机器人学导论》,《人工智能》,《并行计算机互连网络技术》,《C程序设计基础教程》,《计算机组织与结构》    等。</div>
</div>
<script>
$(function(){$(".knnlike a").click(function(){return moreurl(this,{'from':'knnlike'})})})
</script>

  












<div id="rec-ebook-section" class="block5 subject_show">
  

  
  

  <h2>
    <span class="">喜欢读&#34;统计自然语言处理基础&#34;的人也喜欢的电子书</span>
      ······

  </h2>


  <div class="tips-mod">
    支持 Web、iPhone、iPad、Android 阅读器
  </div>
  <div class="content clearfix">
      
      <dl>
        <dt>
          <a href="https://read.douban.com/ebook/1531222/?dcs=subject-rec&amp;dcm=douban&amp;dct=1224802" target="_blank">
            <span class="cover-outer">
              <img src="https://img3.doubanio.com/view/ark_article_cover/retina/public/1531222.jpg?v=1395394360.0">
            </span>
          </a>
        </dt>
        <dd>
          <div class="title">
              <a href="https://read.douban.com/ebook/1531222/" target="_blank">MacTalk·人生元编程</a>
          </div>
          <div class="price">
              2.99元
          </div>
        </dd>
      </dl>
      
      <dl>
        <dt>
          <a href="https://read.douban.com/ebook/500486/?dcs=subject-rec&amp;dcm=douban&amp;dct=1224802" target="_blank">
            <span class="cover-outer">
              <img src="https://img3.doubanio.com/view/ark_article_cover/retina/public/500486.jpg?v=1395393506.0">
            </span>
          </a>
        </dt>
        <dd>
          <div class="title">
              <a href="https://read.douban.com/ebook/500486/" target="_blank">打造Facebook</a>
          </div>
          <div class="price">
              15.00元
          </div>
        </dd>
      </dl>
      
      <dl>
        <dt>
          <a href="https://read.douban.com/ebook/1096463/?dcs=subject-rec&amp;dcm=douban&amp;dct=1224802" target="_blank">
            <span class="cover-outer">
              <img src="https://img3.doubanio.com/view/ark_article_cover/retina/public/1096463.jpg?v=1395394524.0">
            </span>
          </a>
        </dt>
        <dd>
          <div class="title">
              <a href="https://read.douban.com/ebook/1096463/" target="_blank">上帝掷骰子吗——量子物理史话</a>
          </div>
          <div class="price">
              5.99元
          </div>
        </dd>
      </dl>
      
      <dl>
        <dt>
          <a href="https://read.douban.com/ebook/1210652/?dcs=subject-rec&amp;dcm=douban&amp;dct=1224802" target="_blank">
            <span class="cover-outer">
              <img src="https://img3.doubanio.com/view/ark_article_cover/retina/public/1210652.jpg?v=1395394656.0">
            </span>
          </a>
        </dt>
        <dd>
          <div class="title">
              <a href="https://read.douban.com/ebook/1210652/" target="_blank">神经漫游者</a>
          </div>
          <div class="price">
              7.99元
          </div>
        </dd>
      </dl>
      
      <dl>
        <dt>
          <a href="https://read.douban.com/ebook/231420/?dcs=subject-rec&amp;dcm=douban&amp;dct=1224802" target="_blank">
            <span class="cover-outer">
              <img src="https://img3.doubanio.com/view/ark_article_cover/retina/public/231420.jpg?v=1395395960.0">
            </span>
          </a>
        </dt>
        <dd>
          <div class="title">
              <a href="https://read.douban.com/ebook/231420/" target="_blank">反西游记</a>
          </div>
          <div class="price">
              6.99元
          </div>
        </dd>
      </dl>
  </div>
</div>

<div id="db-rec-section" class="block5 subject_show knnlike">
  
  
  

  <h2>
    <span class="">喜欢读&#34;统计自然语言处理基础&#34;的人也喜欢</span>
      ······

  </h2>


  <div class="content clearfix">
      
      <dl class="">
        <dt>
            <a href="https://book.douban.com/subject/1390499/" onclick="moreurl(this, {'total': 10, 'clicked': '1390499', 'pos': 0, 'identifier': 'book-rec-books'})"><img class="m_sub_img" src="https://img1.doubanio.com/lpic/s26018927.jpg"/></a>
        </dt>
        <dd>
          <a href="https://book.douban.com/subject/1390499/" onclick="moreurl(this, {'total': 10, 'clicked': '1390499', 'pos': 0, 'identifier': 'book-rec-books'})" class="">
            自然语言处理综论
          </a>
        </dd>
      </dl>
      
      <dl class="">
        <dt>
            <a href="https://book.douban.com/subject/1102235/" onclick="moreurl(this, {'total': 10, 'clicked': '1102235', 'pos': 1, 'identifier': 'book-rec-books'})"><img class="m_sub_img" src="https://img3.doubanio.com/lpic/s6093730.jpg"/></a>
        </dt>
        <dd>
          <a href="https://book.douban.com/subject/1102235/" onclick="moreurl(this, {'total': 10, 'clicked': '1102235', 'pos': 1, 'identifier': 'book-rec-books'})" class="">
            机器学习
          </a>
        </dd>
      </dl>
      
      <dl class="">
        <dt>
            <a href="https://book.douban.com/subject/3076996/" onclick="moreurl(this, {'total': 10, 'clicked': '3076996', 'pos': 2, 'identifier': 'book-rec-books'})"><img class="m_sub_img" src="https://img3.doubanio.com/lpic/s3394465.jpg"/></a>
        </dt>
        <dd>
          <a href="https://book.douban.com/subject/3076996/" onclick="moreurl(this, {'total': 10, 'clicked': '3076996', 'pos': 2, 'identifier': 'book-rec-books'})" class="">
            统计自然语言处理
          </a>
        </dd>
      </dl>
      
      <dl class="">
        <dt>
            <a href="https://book.douban.com/subject/2061116/" onclick="moreurl(this, {'total': 10, 'clicked': '2061116', 'pos': 3, 'identifier': 'book-rec-books'})"><img class="m_sub_img" src="https://img1.doubanio.com/lpic/s4254558.jpg"/></a>
        </dt>
        <dd>
          <a href="https://book.douban.com/subject/2061116/" onclick="moreurl(this, {'total': 10, 'clicked': '2061116', 'pos': 3, 'identifier': 'book-rec-books'})" class="">
            Pattern Recognition and Machine L...
          </a>
        </dd>
      </dl>
      
      <dl class="">
        <dt>
            <a href="https://book.douban.com/subject/1138189/" onclick="moreurl(this, {'total': 10, 'clicked': '1138189', 'pos': 4, 'identifier': 'book-rec-books'})"><img class="m_sub_img" src="https://img1.doubanio.com/lpic/s1576438.jpg"/></a>
        </dt>
        <dd>
          <a href="https://book.douban.com/subject/1138189/" onclick="moreurl(this, {'total': 10, 'clicked': '1138189', 'pos': 4, 'identifier': 'book-rec-books'})" class="">
            模式分类
          </a>
        </dd>
      </dl>
        <dl class="clear"></dl>
      
      <dl class="">
        <dt>
            <a href="https://book.douban.com/subject/1481158/" onclick="moreurl(this, {'total': 10, 'clicked': '1481158', 'pos': 5, 'identifier': 'book-rec-books'})"><img class="m_sub_img" src="https://img3.doubanio.com/lpic/s24511541.jpg"/></a>
        </dt>
        <dd>
          <a href="https://book.douban.com/subject/1481158/" onclick="moreurl(this, {'total': 10, 'clicked': '1481158', 'pos': 5, 'identifier': 'book-rec-books'})" class="">
            搜索引擎
          </a>
        </dd>
      </dl>
      
      <dl class="">
        <dt>
            <a href="https://book.douban.com/subject/3007147/" onclick="moreurl(this, {'total': 10, 'clicked': '3007147', 'pos': 6, 'identifier': 'book-rec-books'})"><img class="m_sub_img" src="https://img3.doubanio.com/lpic/s5876692.jpg"/></a>
        </dt>
        <dd>
          <a href="https://book.douban.com/subject/3007147/" onclick="moreurl(this, {'total': 10, 'clicked': '3007147', 'pos': 6, 'identifier': 'book-rec-books'})" class="">
            非线性优化计算方法
          </a>
        </dd>
      </dl>
      
      <dl class="">
        <dt>
            <a href="https://book.douban.com/subject/1230487/" onclick="moreurl(this, {'total': 10, 'clicked': '1230487', 'pos': 7, 'identifier': 'book-rec-books'})"><img class="m_sub_img" src="https://img1.doubanio.com/lpic/s26237337.jpg"/></a>
        </dt>
        <dd>
          <a href="https://book.douban.com/subject/1230487/" onclick="moreurl(this, {'total': 10, 'clicked': '1230487', 'pos': 7, 'identifier': 'book-rec-books'})" class="">
            人工智能
          </a>
        </dd>
      </dl>
      
      <dl class="">
        <dt>
            <a href="https://book.douban.com/subject/3625400/" onclick="moreurl(this, {'total': 10, 'clicked': '3625400', 'pos': 8, 'identifier': 'book-rec-books'})"><img class="m_sub_img" src="https://img3.doubanio.com/lpic/s3686710.jpg"/></a>
        </dt>
        <dd>
          <a href="https://book.douban.com/subject/3625400/" onclick="moreurl(this, {'total': 10, 'clicked': '3625400', 'pos': 8, 'identifier': 'book-rec-books'})" class="">
            Dependency Parsing
          </a>
        </dd>
      </dl>
      
      <dl class="">
        <dt>
            <a href="https://book.douban.com/subject/5252170/" onclick="moreurl(this, {'total': 10, 'clicked': '5252170', 'pos': 9, 'identifier': 'book-rec-books'})"><img class="m_sub_img" src="https://img1.doubanio.com/lpic/s4462767.jpg"/></a>
        </dt>
        <dd>
          <a href="https://book.douban.com/subject/5252170/" onclick="moreurl(this, {'total': 10, 'clicked': '5252170', 'pos': 9, 'identifier': 'book-rec-books'})" class="">
            信息检索导论
          </a>
        </dd>
      </dl>
        <dl class="clear"></dl>
  </div>
</div>

  






    <link rel="stylesheet" href="https://img3.doubanio.com/f/book/8f539f4edfa7786707fc6f654e1f373c62a70128/css/book/subject/comment.css"/>
    <div class="mod-hd">
        

        <a class="redbutt j a_show_login rr" href="https://www.douban.com/register?reason=review" rel="nofollow">
            <span> 我来说两句 </span>
        </a>

            
  

  <h2>
    <span class="">短评</span>
      ······
      <span class="pl">&nbsp;(
          <a href="https://book.douban.com/subject/1224802/comments/">全部 30 条</a>
        ) </span>

  </h2>


    </div>
    <div class="nav-tab">
        
    <div class="tabs-wrapper  line">
        <a class="short-comment-tabs on-tab" href="hot" data-tab="hot">热门</a>
        <span>/</span>
        <a class="short-comment-tabs " href="new" data-tab="new">最新</a>
        <span>/</span>
        <a class="j a_show_login " href="follows" data-tab="follows">好友</a>
    </div>

    </div>
    <div id="comment-list-wrapper" class="indent">
        

<div id="comments" class="comment-list hot show">
        <ul>
                
    <li class="comment-item" data-cid="852738139">
        <div class="comment">
            <h3>
                <span class="comment-vote">
                    <span id="c-852738139" class="vote-count">0</span>
                        <a href="javascript:;" id="btn-852738139" class="j a_show_login" data-cid="852738139">有用</a>
                </span>
                <span class="comment-info">
                    <a href="https://www.douban.com/people/xuyichen8417/">紫狐狸</a>
                        <span class="user-stars allstar50 rating" title="力荐"></span>
                    <span>2014-10-10</span>
                </span>
            </h3>
            <p class="comment-content">经典入门书，就是比较老了。</p>
        </div>
    </li>

                
    <li class="comment-item" data-cid="880113665">
        <div class="comment">
            <h3>
                <span class="comment-vote">
                    <span id="c-880113665" class="vote-count">0</span>
                        <a href="javascript:;" id="btn-880113665" class="j a_show_login" data-cid="880113665">有用</a>
                </span>
                <span class="comment-info">
                    <a href="https://www.douban.com/people/48222509/">Abner</a>
                        <span class="user-stars allstar50 rating" title="力荐"></span>
                    <span>2015-01-07</span>
                </span>
            </h3>
            <p class="comment-content">只能说大概翻了一遍，真心把好几本书的统计和信息技术综合了，不错的书</p>
        </div>
    </li>

                
    <li class="comment-item" data-cid="28319113">
        <div class="comment">
            <h3>
                <span class="comment-vote">
                    <span id="c-28319113" class="vote-count">0</span>
                        <a href="javascript:;" id="btn-28319113" class="j a_show_login" data-cid="28319113">有用</a>
                </span>
                <span class="comment-info">
                    <a href="https://www.douban.com/people/wentrue/">Once</a>
                        <span class="user-stars allstar40 rating" title="推荐"></span>
                    <span>2008-01-09</span>
                </span>
            </h3>
            <p class="comment-content">并没有全部看完，有些地方还看不懂。但我知道它挺不错，特别是对于NLP的入门或进阶者</p>
        </div>
    </li>

                
    <li class="comment-item" data-cid="476707092">
        <div class="comment">
            <h3>
                <span class="comment-vote">
                    <span id="c-476707092" class="vote-count">0</span>
                        <a href="javascript:;" id="btn-476707092" class="j a_show_login" data-cid="476707092">有用</a>
                </span>
                <span class="comment-info">
                    <a href="https://www.douban.com/people/lacozhang/">lacozhang</a>
                        <span class="user-stars allstar40 rating" title="推荐"></span>
                    <span>2012-01-01</span>
                </span>
            </h3>
            <p class="comment-content">简短介绍，刚刚入门啊</p>
        </div>
    </li>

                
    <li class="comment-item" data-cid="190945634">
        <div class="comment">
            <h3>
                <span class="comment-vote">
                    <span id="c-190945634" class="vote-count">0</span>
                        <a href="javascript:;" id="btn-190945634" class="j a_show_login" data-cid="190945634">有用</a>
                </span>
                <span class="comment-info">
                    <a href="https://www.douban.com/people/youwillwin/">玉狐狸</a>
                    <span>2011-03-04</span>
                </span>
            </h3>
            <p class="comment-content">上完了一学期的computational linguistic的课 就当我读过吧……</p>
        </div>
    </li>


        </ul>
</div>

        

<div id="comments" class="comment-list new noshow">
        <ul>
                
    <li class="comment-item" data-cid="919786582">
        <div class="comment">
            <h3>
                <span class="comment-vote">
                    <span id="c-919786582" class="vote-count">0</span>
                        <a href="javascript:;" id="btn-919786582" class="j a_show_login" data-cid="919786582">有用</a>
                </span>
                <span class="comment-info">
                    <a href="https://www.douban.com/people/chenzhehuai/">Zhehuai Chen</a>
                        <span class="user-stars allstar40 rating" title="推荐"></span>
                    <span>2015-05-17</span>
                </span>
            </h3>
            <p class="comment-content">挺好的。。。显蓝没看完- - ps求过</p>
        </div>
    </li>

                
    <li class="comment-item" data-cid="880113665">
        <div class="comment">
            <h3>
                <span class="comment-vote">
                    <span id="c-880113665" class="vote-count">0</span>
                        <a href="javascript:;" id="btn-880113665" class="j a_show_login" data-cid="880113665">有用</a>
                </span>
                <span class="comment-info">
                    <a href="https://www.douban.com/people/48222509/">Abner</a>
                        <span class="user-stars allstar50 rating" title="力荐"></span>
                    <span>2015-01-07</span>
                </span>
            </h3>
            <p class="comment-content">只能说大概翻了一遍，真心把好几本书的统计和信息技术综合了，不错的书</p>
        </div>
    </li>

                
    <li class="comment-item" data-cid="852738139">
        <div class="comment">
            <h3>
                <span class="comment-vote">
                    <span id="c-852738139" class="vote-count">0</span>
                        <a href="javascript:;" id="btn-852738139" class="j a_show_login" data-cid="852738139">有用</a>
                </span>
                <span class="comment-info">
                    <a href="https://www.douban.com/people/xuyichen8417/">紫狐狸</a>
                        <span class="user-stars allstar50 rating" title="力荐"></span>
                    <span>2014-10-10</span>
                </span>
            </h3>
            <p class="comment-content">经典入门书，就是比较老了。</p>
        </div>
    </li>

                
    <li class="comment-item" data-cid="770550872">
        <div class="comment">
            <h3>
                <span class="comment-vote">
                    <span id="c-770550872" class="vote-count">0</span>
                        <a href="javascript:;" id="btn-770550872" class="j a_show_login" data-cid="770550872">有用</a>
                </span>
                <span class="comment-info">
                    <a href="https://www.douban.com/people/3916419/">angel</a>
                        <span class="user-stars allstar50 rating" title="力荐"></span>
                    <span>2014-01-26</span>
                </span>
            </h3>
            <p class="comment-content">还是比较经典的，附带看看宗成庆那本</p>
        </div>
    </li>

                
    <li class="comment-item" data-cid="740310132">
        <div class="comment">
            <h3>
                <span class="comment-vote">
                    <span id="c-740310132" class="vote-count">0</span>
                        <a href="javascript:;" id="btn-740310132" class="j a_show_login" data-cid="740310132">有用</a>
                </span>
                <span class="comment-info">
                    <a href="https://www.douban.com/people/3310858/">[已注销]</a>
                        <span class="user-stars allstar50 rating" title="力荐"></span>
                    <span>2014-01-03</span>
                </span>
            </h3>
            <p class="comment-content">第一遍没全看懂，还是回炉一遍好了</p>
        </div>
    </li>


        </ul>
</div>

    </div>
        <p>&gt; <a href="https://book.douban.com/subject/1224802/comments/">更多短评 30 条</a></p>
    <script src="https://img3.doubanio.com/f/book/1d9c8cf5e04fc6d9cae8f10e358c6a6cd458f9b8/js/book/subject/short_comment_vote.js"></script>
    <script src="https://img3.doubanio.com/f/book/39eace58cab8aaeec45a44e878bf0ed06f2ed0a4/js/book/subject/short_comment_nav.js"></script>
    <script>
        (function(){
            $('.comment-list').delegate('.vote-comment', 'click', function(e) {
                vote_comment(e);
            }).delegate('.delete-comment', 'click', function(e) {
                if (confirm('确定删除吗？')) {
                    delete_comment(e);
                }
            });
        })();
    </script>

  

<link rel="stylesheet" href="https://img3.doubanio.com/misc/mixed_static/7f54ef457c36133a.css">

<section class="topics mod">
    <header>
        <h2>
            统计自然语言处理基础的话题 · · · · · ·
            <span class="pl">( <span class="gallery_topics">全部 <span id="topic-count"></span> 条</span> )</span>
        </h2>
    </header>

    




<section class="subject-topics">
    <div class="topic-guide" id="topic-guide">
        <img class="ic_question" src="//img3.doubanio.com/f/ithildin/b1a3edea3d04805f899e9d77c0bfc0d158df10d5/pics/export/icon_question.png">
        <div class="tip_content">
            <div class="tip_title">什么是话题</div>
            <div class="tip_desc">
                <div>无论是一部作品、一个人，还是一件事，都往往可以衍生出许多不同的话题。将这些话题细分出来，分别进行讨论，会有更多收获。</div>
            </div>
        </div>
        <img class="ic_guide" src="//img3.doubanio.com/f/ithildin/529f46d86bc08f55cd0b1843d0492242ebbd22de/pics/export/icon_guide_arrow.png">
        <img class="ic_close" id="topic-guide-close" src="//img3.doubanio.com/f/ithildin/2eb4ad488cb0854644b23f20b6fa312404429589/pics/export/close@3x.png">
    </div>

    <div id="topic-items"></div>

    <script>
        window.subject_id = 1224802;
        window.join_label_text = '写书评参与';

        window.topic_display_count = 4;
        window.topic_item_display_count = 1;
        window.no_content_fun_call_name = "no_topic";

        window.guideNode = document.getElementById('topic-guide');
        window.guideNodeClose = document.getElementById('topic-guide-close');
    </script>
    
        <link rel="stylesheet" href="https://img3.doubanio.com/f/ithildin/f731c9ea474da58c516290b3a6b1dd1237c07c5e/css/export/subject_topics.css">
        <script src="https://img3.doubanio.com/f/ithildin/d3590fc6ac47b33c804037a1aa7eec49075428c8/js/export/moment-with-locales-only-zh.js"></script>
        <script src="https://img3.doubanio.com/f/ithildin/936839eb18719d63bdab8637feaa21b4e6d72d8e/js/export/subject_topics.es6"></script>

</section>

    <script>
        function no_topic(){
            $('#content .topics').remove();
        }
    </script>
</section>

<section class="reviews mod book-content">
    <header>
        <a href="new_review" rel="nofollow" class="create-review redbutt rr" 
            data-isverify="False"
            data-verify-url="https://www.douban.com/accounts/phone/verify?redir=https://book.douban.com/subject/1224802/new_review">
            <span>我要写书评</span>
        </a>
        <h2>
            统计自然语言处理基础的书评 · · · · · ·
            <span class="pl">( <a href="reviews">全部 5 条</a> )</span>
        </h2>
    </header>

    

<style>
#gallery-topics-selection {
  display: none;
  position: fixed;
  width: 595px;
  padding: 40px 40px 33px 40px;
  background: #fff;
  border-radius: 10px;
  box-shadow: 0 2px 16px 0 rgba(0, 0, 0, 0.2);
  top: 50%;
  left: 50%;
  -webkit-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
  z-index: 9999;
}
#gallery-topics-selection h1 {
  font-size: 18px;
  color: #007722;
  margin-bottom: 36px;
  padding: 0;
  line-height: 28px;
  font-weight: normal;
}
#gallery-topics-selection .gl_topics {
  border-bottom: 1px solid #dfdfdf;
  max-height: 298px;
  overflow-y: scroll;
}
#gallery-topics-selection .topic {
  margin-bottom: 24px;
}
#gallery-topics-selection .topic_name {
  font-size: 15px;
  color: #333;
  margin: 0;
  line-height: inherit;
}
#gallery-topics-selection .topic_meta {
  font-size: 13px;
  color: #999;
}
#gallery-topics-selection .topics_skip {
  display: block;
  cursor: pointer;
  font-size: 16px;
  color: #3377AA;
  text-align: center;
  margin-top: 33px;
}
#gallery-topics-selection .topics_skip:hover {
  background: transparent;
}
#gallery-topics-selection .close_selection {
  position: absolute;
  width: 30px;
  height: 20px;
  top: 46px;
  right: 40px;
  background: #fff;
  color: #999;
  text-align: right;
}
#gallery-topics-selection .close_selection:hover{
  background: #fff;
  color: #999;
}
</style>

<div id="gallery-topics-selection"></div>




        <div class="review_filter">
            <a href="javascript:;;" class="cur" data-sort="">热门</a href="javascript:;;"> / 
            <a href="javascript:;;" data-sort="time">最新</a href="javascript:;;"> / 
            <a href="javascript:;;" data-sort="follow">好友</a href="javascript:;;">
            
        </div>


        



<div class="review-list  ">
        
    
    
        
    
    <div xmlns:v="http://rdf.data-vocabulary.org/#" typeof="v:Review" data-cid="3322758">
        <div class="main review-item" id="3322758">
        
            
    
    <header class="main-hd">
        <a href="https://www.douban.com/people/chengsshi/" class="avator">
            <img width="24" height="24" src="https://img1.doubanio.com/icon/u1371770-17.jpg">
        </a>

        <a href="https://www.douban.com/people/chengsshi/" property="v:reviewer" class="name">终南</a>

            <span property="v:rating" class="allstar30 main-title-rating" title="还行"></span>

        <span property="v:dtreviewed" content="2010-06-10" class="main-meta">2010-06-10 22:15:07</span>
        

    </header>

        
            <div class="main-bd">
                
                <h2><a href="https://book.douban.com/review/3322758/">翻译问题</a></h2>

                <div id="review_3322758_short" class="review-short" data-rid="3322758">
                    <div class="short-content">

                        P17（中文版） English：The significance of power laws 中文：强法则的重要性  power law：指数法則，幂律  

                        &nbsp;(<a href="javascript:;" id="toggle-3322758-copy" class="unfold" title="展开">展开</a>)
                    </div>
                </div>
            
                <div id="review_3322758_full" class="hidden">
                    <div id="review_3322758_full_content" class="full-content"></div>
                </div>

                <div class="action">
                    <a href="javascript:;;" class="action-btn up" data-rid="3322758" title="有用">
                        <img src="https://img3.doubanio.com/f/zerkalo/536fd337139250b5fb3cf9e79cb65c6193f8b20b/pics/up.png" />
                        <span id="r-useful_count-3322758">
                                4
                        </span>
                    </a>
                    <a href="javascript:;;" class="action-btn down" data-rid="3322758" title="没用">
                        <img src="https://img3.doubanio.com/f/zerkalo/68849027911140623cf338c9845893c4566db851/pics/down.png" />
                        <span id="r-useless_count-3322758">
                        </span>
                    </a>
                    <a href="https://book.douban.com/review/3322758/#comments" class="reply">3回应</a>

                    <a href="javascript:;;" class="fold hidden">收起</a>
                </div>
            </div>
        </div>
    </div>

        
    
    <div xmlns:v="http://rdf.data-vocabulary.org/#" typeof="v:Review" data-cid="7462956">
        <div class="main review-item" id="7462956">
        
            
    
    <header class="main-hd">
        <a href="https://www.douban.com/people/frankwang/" class="avator">
            <img width="24" height="24" src="https://img3.doubanio.com/icon/u1558440-45.jpg">
        </a>

        <a href="https://www.douban.com/people/frankwang/" property="v:reviewer" class="name">黠之大者</a>

            <span property="v:rating" class="allstar40 main-title-rating" title="推荐"></span>

        <span property="v:dtreviewed" content="2015-05-04" class="main-meta">2015-05-04 12:21:50</span>
        

    </header>

        
            <div class="main-bd">
                
                <h2><a href="https://book.douban.com/review/7462956/">区分n-gram数量B与词汇量V</a></h2>

                <div id="review_7462956_short" class="review-short" data-rid="7462956">
                    <div class="short-content">

                         这本书当中依然有很多错误，译者也助长了错误。在第六章 语言模型部分，作者详细定义了各种概念，但是对于B的翻译不够好：训练实例的类别量，其实就是模型的参数数量或者n-gram的数量。围绕这个概念问题，出了一系列错误。  第一个错误表现在127页的译者注释，译者注意到manni...

                        &nbsp;(<a href="javascript:;" id="toggle-7462956-copy" class="unfold" title="展开">展开</a>)
                    </div>
                </div>
            
                <div id="review_7462956_full" class="hidden">
                    <div id="review_7462956_full_content" class="full-content"></div>
                </div>

                <div class="action">
                    <a href="javascript:;;" class="action-btn up" data-rid="7462956" title="有用">
                        <img src="https://img3.doubanio.com/f/zerkalo/536fd337139250b5fb3cf9e79cb65c6193f8b20b/pics/up.png" />
                        <span id="r-useful_count-7462956">
                                3
                        </span>
                    </a>
                    <a href="javascript:;;" class="action-btn down" data-rid="7462956" title="没用">
                        <img src="https://img3.doubanio.com/f/zerkalo/68849027911140623cf338c9845893c4566db851/pics/down.png" />
                        <span id="r-useless_count-7462956">
                        </span>
                    </a>
                    <a href="https://book.douban.com/review/7462956/#comments" class="reply">0回应</a>

                    <a href="javascript:;;" class="fold hidden">收起</a>
                </div>
            </div>
        </div>
    </div>

        
    
    <div xmlns:v="http://rdf.data-vocabulary.org/#" typeof="v:Review" data-cid="5330949">
        <div class="main review-item" id="5330949">
        
            
    
    <header class="main-hd">
        <a href="https://www.douban.com/people/realplayer-z/" class="avator">
            <img width="24" height="24" src="https://img3.doubanio.com/icon/u3680654-1.jpg">
        </a>

        <a href="https://www.douban.com/people/realplayer-z/" property="v:reviewer" class="name">realplayer-z</a>

            <span property="v:rating" class="allstar20 main-title-rating" title="较差"></span>

        <span property="v:dtreviewed" content="2012-03-04" class="main-meta">2012-03-04 18:00:16</span>
        

    </header>

        
            <div class="main-bd">
                
                <h2><a href="https://book.douban.com/review/5330949/">翻译很差，原书内容不错</a></h2>

                <div id="review_5330949_short" class="review-short" data-rid="5330949">
                    <div class="short-content">

                        power law译成强法则，perplexity译成混乱度，碰到稍难一点的句子居然直接跳过不译，狂汗。 现在还没看多少，感觉原书内容还是不错的，叙述比较完备，就是英文写得稍微难了点，不是特别简单易懂的写法。

                        &nbsp;(<a href="javascript:;" id="toggle-5330949-copy" class="unfold" title="展开">展开</a>)
                    </div>
                </div>
            
                <div id="review_5330949_full" class="hidden">
                    <div id="review_5330949_full_content" class="full-content"></div>
                </div>

                <div class="action">
                    <a href="javascript:;;" class="action-btn up" data-rid="5330949" title="有用">
                        <img src="https://img3.doubanio.com/f/zerkalo/536fd337139250b5fb3cf9e79cb65c6193f8b20b/pics/up.png" />
                        <span id="r-useful_count-5330949">
                                1
                        </span>
                    </a>
                    <a href="javascript:;;" class="action-btn down" data-rid="5330949" title="没用">
                        <img src="https://img3.doubanio.com/f/zerkalo/68849027911140623cf338c9845893c4566db851/pics/down.png" />
                        <span id="r-useless_count-5330949">
                        </span>
                    </a>
                    <a href="https://book.douban.com/review/5330949/#comments" class="reply">0回应</a>

                    <a href="javascript:;;" class="fold hidden">收起</a>
                </div>
            </div>
        </div>
    </div>

        
    
    <div xmlns:v="http://rdf.data-vocabulary.org/#" typeof="v:Review" data-cid="5478456">
        <div class="main review-item" id="5478456">
        
            
    
    <header class="main-hd">
        <a href="https://www.douban.com/people/61192926/" class="avator">
            <img width="24" height="24" src="https://img3.doubanio.com/icon/u61192926-1.jpg">
        </a>

        <a href="https://www.douban.com/people/61192926/" property="v:reviewer" class="name">math007_地球物</a>

            <span property="v:rating" class="allstar30 main-title-rating" title="还行"></span>

        <span property="v:dtreviewed" content="2012-06-25" class="main-meta">2012-06-25 00:15:04</span>
        

    </header>

        
            <div class="main-bd">
                
                <h2><a href="https://book.douban.com/review/5478456/">书让人有点失望</a></h2>

                <div id="review_5478456_short" class="review-short" data-rid="5478456">
                    <div class="short-content">
                            <p class="spoiler-tip">这篇书评可能有关键情节透露</p>

                        还行，但比想象的要差。  缺点： 书翻译的很蹩脚，写得也有些蹩脚。 书里充满了概念。 一个特点：文字多的地方，基本感觉易读性比较差，说来说去不知道在说什么了。公式多的反而好理解一些。 不该省略的地方省略了不少。比如2.1.10 贝叶斯，贝叶斯大学学过的，但是33页那个讲的...

                        &nbsp;(<a href="javascript:;" id="toggle-5478456-copy" class="unfold" title="展开">展开</a>)
                    </div>
                </div>
            
                <div id="review_5478456_full" class="hidden">
                    <div id="review_5478456_full_content" class="full-content"></div>
                </div>

                <div class="action">
                    <a href="javascript:;;" class="action-btn up" data-rid="5478456" title="有用">
                        <img src="https://img3.doubanio.com/f/zerkalo/536fd337139250b5fb3cf9e79cb65c6193f8b20b/pics/up.png" />
                        <span id="r-useful_count-5478456">
                        </span>
                    </a>
                    <a href="javascript:;;" class="action-btn down" data-rid="5478456" title="没用">
                        <img src="https://img3.doubanio.com/f/zerkalo/68849027911140623cf338c9845893c4566db851/pics/down.png" />
                        <span id="r-useless_count-5478456">
                        </span>
                    </a>
                    <a href="https://book.douban.com/review/5478456/#comments" class="reply">0回应</a>

                    <a href="javascript:;;" class="fold hidden">收起</a>
                </div>
            </div>
        </div>
    </div>

        
    
    <div xmlns:v="http://rdf.data-vocabulary.org/#" typeof="v:Review" data-cid="1476258">
        <div class="main review-item" id="1476258">
        
            
    
    <header class="main-hd">
        <a href="https://www.douban.com/people/lslove/" class="avator">
            <img width="24" height="24" src="https://img3.doubanio.com/icon/u2732723-10.jpg">
        </a>

        <a href="https://www.douban.com/people/lslove/" property="v:reviewer" class="name">盐汤儿</a>

            <span property="v:rating" class="allstar50 main-title-rating" title="力荐"></span>

        <span property="v:dtreviewed" content="2008-08-21" class="main-meta">2008-08-21 17:59:34</span>
        

    </header>

        
            <div class="main-bd">
                
                <h2><a href="https://book.douban.com/review/1476258/">SNLP的入门教程</a></h2>

                <div id="review_1476258_short" class="review-short" data-rid="1476258">
                    <div class="short-content">

                        这本书不是很厚，也没有自然语言处理综论介绍的全面。但就想要学习SNLP的人来说相当不错。  同时书中除了自然语言处理中传统的如分词、标注等领域之外，在最后也涉及到了一些较为新型和更为交叉的领域。从SNLP这一领域做出了很好的诠释！

                        &nbsp;(<a href="javascript:;" id="toggle-1476258-copy" class="unfold" title="展开">展开</a>)
                    </div>
                </div>
            
                <div id="review_1476258_full" class="hidden">
                    <div id="review_1476258_full_content" class="full-content"></div>
                </div>

                <div class="action">
                    <a href="javascript:;;" class="action-btn up" data-rid="1476258" title="有用">
                        <img src="https://img3.doubanio.com/f/zerkalo/536fd337139250b5fb3cf9e79cb65c6193f8b20b/pics/up.png" />
                        <span id="r-useful_count-1476258">
                                1
                        </span>
                    </a>
                    <a href="javascript:;;" class="action-btn down" data-rid="1476258" title="没用">
                        <img src="https://img3.doubanio.com/f/zerkalo/68849027911140623cf338c9845893c4566db851/pics/down.png" />
                        <span id="r-useless_count-1476258">
                                1
                        </span>
                    </a>
                    <a href="https://book.douban.com/review/1476258/#comments" class="reply">0回应</a>

                    <a href="javascript:;;" class="fold hidden">收起</a>
                </div>
            </div>
        </div>
    </div>




    

    

    <script type="text/javascript">
    ;(function(){
        window.subject_id = window.subject_id || 1224802;
        "use strict";

var _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ("value" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

function _possibleConstructorReturn(self, call) { if (!self) { throw new ReferenceError("this hasn't been initialised - super() hasn't been called"); } return call && (typeof call === "object" || typeof call === "function") ? call : self; }

function _inherits(subClass, superClass) { if (typeof superClass !== "function" && superClass !== null) { throw new TypeError("Super expression must either be null or a function, not " + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }

var GalleryTopicsSelection = function (_React$Component) {
  _inherits(GalleryTopicsSelection, _React$Component);

  function GalleryTopicsSelection() {
    _classCallCheck(this, GalleryTopicsSelection);

    var _this = _possibleConstructorReturn(this, (GalleryTopicsSelection.__proto__ || Object.getPrototypeOf(GalleryTopicsSelection)).call(this));

    _this.state = {
      topics: []
    };
    return _this;
  }

  _createClass(GalleryTopicsSelection, [{
    key: "componentDidMount",
    value: function componentDidMount() {
      var _this2 = this;

      $.ajax({
        url: "https://m.douban.com/rexxar/api/v2/gallery/subject_feed?start=0&subject_id=" + window.subject_id + "&ck=" + get_cookie('ck'),
        xhrFields: { withCredentials: true },
        success: function success(res) {
          _this2.setState({
            topics: res.items
          });
          if (res.total) {
            window.has_gallery_topics = true;
          }
        }
      });
    }
  }, {
    key: "render",
    value: function render() {
      var topics = this.state.topics;

      return React.createElement(
        "section",
        null,
        React.createElement(
          "a",
          { href: "new_review?from=gallery-topics-selection&click=close", rel: "nofollow", className: "close_selection" },
          "\u8DF3\u8FC7"
        ),
        React.createElement(
          "h1",
          null,
          "\u4E0B\u9762\u662F\u5426\u6709\u4F60\u60F3\u5199\u7684\u8BDD\u9898\uFF1F"
        ),
        topics.length ? React.createElement(
          "ul",
          { className: "gl_topics" },
          topics.map(function (item) {
            return React.createElement(
              "li",
              { className: "topic" },
              React.createElement(
                "a",
                { href: "new_review?from=gallery-topics-selection&click=create&tag_from_gallery=" + item.topic.name,
                  className: "comment_btn write_review",
                  rel: "nofollow" },
                React.createElement("img", { src: window.write_icon }),
                React.createElement(
                  "span",
                  null,
                  window.join_label_text
                )
              ),
              React.createElement(
                "h2",
                { className: "topic_name" },
                item.topic.name
              ),
              React.createElement(
                "div",
                { className: "topic_meta" },
                item.topic.card_subtitle
              )
            );
          })
        ) : React.createElement(
          "div",
          null,
          "\u52A0\u8F7D\u4E2D"
        ),
        React.createElement(
          "a",
          { href: "new_review?from=gallery-topics-selection&click=skip", rel: "nofollow", className: "topics_skip" },
          React.createElement(
            "span",
            null,
            "\u4E0A\u9762\u6CA1\u6709\u6211\u60F3\u5199\u7684\u8BDD\u9898\uFF0C\u53BB\u5199\u5F71\u8BC4 \uFF1E "
          )
        )
      );
    }
  }]);

  return GalleryTopicsSelection;
}(React.Component);

$(function () {
  var glRoot = document.getElementById('gallery-topics-selection');

  ReactDOM.render(React.createElement(GalleryTopicsSelection, null), glRoot);
});;
    })();
</script><script type="text/javascript" src="https://img3.doubanio.com/misc/mixed_static/289d9eab6b43a9ec.js"></script>
    <!-- COLLECTED CSS -->
</div>







            <p class="pl">
                &gt;
                <a href="reviews">
                    更多书评5篇
                </a>
            </p>
</section>
    
<!-- COLLECTED JS -->

  









<div class="ugc-mod reading-notes">
  <div class="hd">
    <h2>
      读书笔记&nbsp;&nbsp;· · · · · ·&nbsp;
      
    </h2>

      <a class="redbutt rr j a_show_login" href="https://www.douban.com/register?reason=annotate" rel="nofollow"><span>我来写笔记</span></a>
  </div>
  

    <div class="bd">
      <ul class="inline-tabs">
        <li class="on"><a href="#rank" id="by_rank" >按有用程度</a></li>
        <li><a href="#page" id="by_page" >按页码先后</a></li>
        <li><a href="#time" id="by_time" >最新笔记</a></li>
      </ul>
      
  <ul class="comments by_rank" >
      
      <li class="ctsh clearfix" data-cid="34999032">
        <div class="ilst">
          <a href="https://www.douban.com/people/frankwang/"><img src="https://img3.doubanio.com/icon/u1558440-45.jpg" alt="黠之大者" class="" /></a>
        </div>
        <div class="con">
          <div class="nlst">
            <h3>
              <div class="note-toggle rr">
                <a href="https://book.douban.com/annotation/34999032/" class="note-unfolder">展开</a>
                <a href="javascript:void(0);" class="note-folder">收起</a>
              </div>
              <a href="https://book.douban.com/annotation/34999032/" class="">第121页</a></h3>
          </div>
          <div class="clst">
            <p class="user"><a href="https://www.douban.com/people/frankwang/" class=" " title="黠之大者">黠之大者</a>
                (I am serious with python)
              
            </p>
            <div class="reading-note" data-cid="34999032">
              <div class="short">
                
                  <span class="">区分n-gram数量B与词汇量V

这本书当中依然有很多错误，译者也助长了错误。在第六章 语言模型部分，作者详细定义了各种概念，但是对于B的翻译不够好：训练实例的类别量，其实就是模型的参数数量或者n-gram的数量。围绕这个概念问题，出了一系列错误。

第一个错误表现在127页的译者注释，译者注意到manning公布的勘误表，注意到：“训练语料有273266个词形，B应该是273266，。。。译者注”。

这里的B应该是V，对于二元语法..</span>
                <p class="pl">
                  <span class="">2015-05-04 12:05</span>
                  
                </p>
              </div>
              <div class="all hidden" style="display:none" >
                区分n-gram数量B与词汇量V<div style="padding-bottom:1em;"></div>这本书当中依然有很多错误，译者也助长了错误。在第六章 语言模型部分，作者详细定义了各种概念，但是对于B的翻译不够好：训练实例的类别量，其实就是模型的参数数量或者n-gram的数量。围绕这个概念问题，出了一系列错误。<div style="padding-bottom:1em;"></div>第一个错误表现在127页的译者注释，译者注意到manning公布的勘误表，注意到：“训练语料有273266个词形，B应该是273266，。。。译者注”。<div style="padding-bottom:1em;"></div>这里的B应该是V，对于二元语法模型B = V^2。<div style="padding-bottom:1em;"></div>第二个错误是128页：“这个训练语料库共有14585个词形。所以对于新的条件概率p(not|was)，新的估计是(608 + 0.5)/(9404 + 14589*0.5)”。这里也跟着错，应该是：<div style="padding-bottom:1em;"></div>(608 + 0.5)/(9404 + 14589^2*0.5)<div style="padding-bottom:1em;"></div>当然这里是由于原作者错误，译者不察觉。相应地，表格6.5里的ELE估计都是错的，原文结论说折扣掉一半也完全错误。<div style="padding-bottom:1em;"></div>结论是第六章出现的一系列错误作者难辞其咎。译者也未能指出错误。<div style="padding-bottom:1em;"></div><a rel="nofollow" href="http://nlp.stanford.edu/fsnlp/errata.html" target="_blank">http://nlp.stanford.edu/fsnlp/errata.html</a><div style="padding-bottom:1em;"></div>page 196, line -13: Change &quot;This will be V^{n-1}&quot; to &quot;This will be V&quot;, given the following major clarification: In Section 6.1, the number of 'bins' is used to refer to the number of possible values of the classificatory feature vectors, while (unfortunately) from Section 6.2 on, with this change, the term 'bins' and the letter B is used to refer to the number of values of the target feature. This is V for prediction of the next word, but V^n for predicting the frequency of n-grams. (Thanks to Tibor Kiss &lt;tibor .... linguistics.ruhr-uni-bochum.de&gt;<div style="padding-bottom:1em;"></div>page 202-203: While the whole corpus had 400,653 word types, the training corpus had only 273,266 word types. This smaller number should have been used as B in the calculation of a Laplace's law estimate of table 6.4 (whereas actually 400,653 was used). The result of this change is that f_{Lap}(0) = 0.000295, and then 99.96% of the probability mass is given to previously unseen bigrams (!). In such a model, note that we have used a (demonstrably wrong) closed vocabulary assumption, so despite this huge mass being given to unseen bigrams, none is being given to potential bigrams using vocabulary items outside the training set vocabulary (OOV = out of vocabulary items). (Thanks to Steve Renals &lt;s.renals .... dcs.shef.ac.uk&gt; and Gary Cottrell &lt;gary .... cs.ucsd.edu&gt;<div style="padding-bottom:1em;"></div>page 205, line 2-3: Correction: here it is said that there are 14589 word types, but the number given elsewhere in the chapter (and the actual number found on rechecking the data file) is 14585. Clarification: Here we directly smooth the conditional distributions, so there are only |V| = 14585 values for the bigram conditional distribution added into the denominator during smoothing, whereas on pp. 202-203, we were estimating bigram probabilities, and there are |V|^2 different bigrams. (Thanks to Hidetosi Sirai &lt;sirai .... sccs.chukyo-u.ac.jp&gt;, Mark Lewellen &lt;lewellen .... erols.com&gt;, and Gary Cottrell &lt;gary .... cs.ucsd.edu&gt;
                  <div class="col-rec-con clearfix">
                    







<div class="rec-sec">

    <span class="rec">

<a href="https://www.douban.com/accounts/register?reason=collect" class="j a_show_login lnk-sharing lnk-douban-sharing">推荐</a>
</span>
</div>

                  </div>
                <div class="pl col-time">
                  <a href="https://book.douban.com/annotation/34999032/#comments">回应</a>&nbsp;&nbsp;
                  2015-05-04 12:05
                </div>
              </div>
            </div>
          </div>
        </div>
      </li>
      
      <li class="ctsh clearfix" data-cid="24558993">
        <div class="ilst">
          <a href="https://www.douban.com/people/ppwwyyxx/"><img src="https://img3.doubanio.com/icon/u33367269-4.jpg" alt="Silvery" class="" /></a>
        </div>
        <div class="con">
          <div class="nlst">
            <h3>
              <div class="note-toggle rr">
                <a href="https://book.douban.com/annotation/24558993/" class="note-unfolder">展开</a>
                <a href="javascript:void(0);" class="note-folder">收起</a>
              </div>
              <a href="https://book.douban.com/annotation/24558993/" class="">第1页</a></h3>
          </div>
          <div class="clst">
            <p class="user"><a href="https://www.douban.com/people/ppwwyyxx/" class=" " title="Silvery">Silvery</a>
                (autumn leaves)
              
                <span class="allstar50" title="力荐"></span>
            </p>
            <div class="reading-note" data-cid="24558993">
              <div class="short">
                
                  <span class="">第一本带了专业方向性的书,做点小记录
前半本有价值的东西不多..
另外..豆瓣上发东西要怎么才能让格式好看点...

#1.绪
=====
	不关心是否合乎语法这一分类

	语言的自我发展:
	19c才出现kind of/sort of的程度修饰用法

	起因:a kind of adj. n. -&gt; (kind of adj.) n.

	利用词性+语法的parse,歧义太多:
	List the sales of the products produced in 1973 with the products produced in 1972
	有455种句法分..</span>
                <p class="pl">
                  <span class="">2013-02-14 00:02</span>
                  
                </p>
              </div>
              <div class="all hidden" style="display:none" >
                第一本带了专业方向性的书,做点小记录<div style="padding-bottom:1em;"></div>前半本有价值的东西不多..<div style="padding-bottom:1em;"></div>另外..豆瓣上发东西要怎么才能让格式好看点...<div style="padding-bottom:1em;"></div>#1.绪<div style="padding-bottom:1em;"></div>=====<div style="padding-bottom:1em;"></div>	不关心是否合乎语法这一分类<div style="padding-bottom:1em;"></div>	语言的自我发展:<div style="padding-bottom:1em;"></div>	19c才出现kind of/sort of的程度修饰用法<div style="padding-bottom:1em;"></div>	起因:a kind of adj. n. -&gt; (kind of adj.) n.<div style="padding-bottom:1em;"></div>	利用词性+语法的parse,歧义太多:<div style="padding-bottom:1em;"></div>	List the sales of the products produced in 1973 with the products produced in 1972<div style="padding-bottom:1em;"></div>	有455种句法分析结果(Martin)<div style="padding-bottom:1em;"></div>	增加限制与优选规则(只能手工添加)非常费力,且无法处理生动的(修辞)语言.<div style="padding-bottom:1em;"></div>	相反,统计方法自动归纳结构信息,挖掘搭配关系.<div style="padding-bottom:1em;"></div>	Garden Pathing现象:<div style="padding-bottom:1em;"></div>	The horse raced past the barn fell.<div style="padding-bottom:1em;"></div>	发现无法分析后要回溯到The horse.在口语中因为语气及停顿,不会有此问题<div style="padding-bottom:1em;"></div>	Brown语料库/Susanne语料库(free)<div style="padding-bottom:1em;"></div>	token- 总词次  type- 总词数<div style="padding-bottom:1em;"></div>	Zipf法则:大型语料库中,一个单词的词频与词频排名成反比.<div style="padding-bottom:1em;"></div>	Entropy: H(x) = -E[log(p(x))], H(X,Y) = -E[log(p(x,y))], H(Y|X) = -E[log (p(y|x))]<div style="padding-bottom:1em;"></div>	(chain rule:) H(X,Y) = H(X) + H(Y|X)<div style="padding-bottom:1em;"></div>	Mutual Information: I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)<div style="padding-bottom:1em;"></div>	已知一者对另一者不确定性的减少量<div style="padding-bottom:1em;"></div>	I(X;Y) = E[log(p(x,y) / p(x)p(y) )]<div style="padding-bottom:1em;"></div>	噪声信道模型: 通过最大化编码的entropy, 可使得噪声信道的输入与输出的互信息最<div style="padding-bottom:1em;"></div>	大化, 此时传输速率可等于信道capacity.<div style="padding-bottom:1em;"></div>	有限记忆性-&gt;k阶Markov链.<div style="padding-bottom:1em;"></div>	用一阶二阶模型估算英语的熵约为3.3-4, Shannon人工实验得到1.34<div style="padding-bottom:1em;"></div>#4.2文本<div style="padding-bottom:1em;"></div>========<div style="padding-bottom:1em;"></div>	文本程序预处理中的许多实际问题(格式与标点相关的识别困	难)<div style="padding-bottom:1em;"></div>#6 N-gram model<div style="padding-bottom:1em;"></div>===============<div style="padding-bottom:1em;"></div>	可以想象一个靠前k个词及谓语的预测会很可靠. 但谓语识别很困难.<div style="padding-bottom:1em;"></div>	trigram模型仍然是非常好的预测模型.<div style="padding-bottom:1em;"></div>	高阶n-gram模型只适合在巨量测试语料上使用.<div style="padding-bottom:1em;"></div>	如何给未登录词赋一个非0概率?<div style="padding-bottom:1em;"></div>	Laplace: 全部分子+1<div style="padding-bottom:1em;"></div>	Lidstone: +t, Jeffreys-perkes: t = 1/2时是期望似然估计<div style="padding-bottom:1em;"></div>	选一个小t以避免太多概率空间转移到未知事件.<div style="padding-bottom:1em;"></div>	6.2.3如何选取训练数据和测试数据<div style="padding-bottom:1em;"></div>	p138网站<div style="padding-bottom:1em;"></div>	利用n-gram的频率给出概率估计的各类技巧:<div style="padding-bottom:1em;"></div>	deleted estimation, deleted interpolation estimation, Good-Turing estimation<div style="padding-bottom:1em;"></div>	6.3:通过不同阶(较小)的n-gram频率来估计给定n值的n-gram概率,可以有助于数据稀疏问题<div style="padding-bottom:1em;"></div>	线性插值:P(wn|wn-2, wn-1) = a1P(wn) + a2P(wn|wn-1) + a3P(wn|wn-1, wn-2)<div style="padding-bottom:1em;"></div>	可利用EM算法(Expectation Maximization)确定最佳权值<div style="padding-bottom:1em;"></div>	Katz回退算法..<div style="padding-bottom:1em;"></div>	一般化线性插值: 系数权值是关于历史的函数<div style="padding-bottom:1em;"></div>#7 语义消岐<div style="padding-bottom:1em;"></div>===========<div style="padding-bottom:1em;"></div>	有监督学习:含语义标注数据. 无监督学习(clustering):聚类<div style="padding-bottom:1em;"></div>	以人类的标注成功率作为效果的上界,将所有词指定为其最常用语义作为其效果的下界<div style="padding-bottom:1em;"></div>	7.2有监督消岐<div style="padding-bottom:1em;"></div>	7.2.1 Bayes Classification<div style="padding-bottom:1em;"></div>	决定w的语义s:<div style="padding-bottom:1em;"></div>	s = max(s_k){P(s_k | c)} = max(s_k)P(c| s_k)P(s_k) (上下文c(context window)的概率为常量)<div style="padding-bottom:1em;"></div>	  = max(s_k){log P(c| s_k) + log P(s_k) }<div style="padding-bottom:1em;"></div>	使用Naive Bayes Assumption, 赋予其独立性,则上式继续<div style="padding-bottom:1em;"></div>	  = max(s_k){log P(s_k) + sum(v in c){log P(v | s_k) } }<div style="padding-bottom:1em;"></div>	其中的P(v|s), P(s)可从语料(必须是已标记好的)中利用MLE计算出(最好加上适当的平滑)<div style="padding-bottom:1em;"></div>	P(v_t|s) = C(v_t, s) / sum(v_i)C(v_i,s); P(s) = C(s) / C(words)<div style="padding-bottom:1em;"></div>	7.2.2 An Approach using Information Theroy<div style="padding-bottom:1em;"></div>	将w可能的语义集合P,与其对应指示器(如下文的两个单词)的可能集合Q做划分<div style="padding-bottom:1em;"></div>	P = {P1, P2, ...}, Q = {Q1, Q2, ...}<div style="padding-bottom:1em;"></div>	使得这样划分后互信息I(P, Q)最大,重复迭代二者可保证解的存在<div style="padding-bottom:1em;"></div>	w = {做} P = {take, make}, Q = {measure, note, decision}<div style="padding-bottom:1em;"></div>	7.3Dict-Based Disambiguation<div style="padding-bottom:1em;"></div>	7.3.1语义定义<div style="padding-bottom:1em;"></div>	判断w的语义时,挑选与w的附近词定义相似性最大的定义(看词语重复)<div style="padding-bottom:1em;"></div>	7.3.2类义<div style="padding-bottom:1em;"></div>	利用包含语义范畴信息的词典,每个词有若干个tag<div style="padding-bottom:1em;"></div>	在运行中可以对遇到的词添加新的tag(Classification)<div style="padding-bottom:1em;"></div>	类义的范畴有时与语义无法对应匹配,因此用于语义消岐效果不好.<div style="padding-bottom:1em;"></div>	7.3.3基于第二语言语料库. 利用目标词的上下文的翻译的上下文寻找结果.<div style="padding-bottom:1em;"></div>	7.3.4文本语义:一词在整篇文本中更可能取同一语义.<div style="padding-bottom:1em;"></div>	7.4无监督消岐<div style="padding-bottom:1em;"></div>	随机初始化P(v| s_k), 根据EM算法重新估计P(v| s_k), 使得模型整体的似然值保持增长<div style="padding-bottom:1em;"></div>#8 词汇获取<div style="padding-bottom:1em;"></div>===========<div style="padding-bottom:1em;"></div>	8.1 关于假设检验,拒真受伪<div style="padding-bottom:1em;"></div>	8.2 动词子范畴帮助句法分析<div style="padding-bottom:1em;"></div>		动词可引导什么样的结构. tell sb sth; find sb adj;<div style="padding-bottom:1em;"></div>		利用这样的范畴作出假设,检验,自我修复<div style="padding-bottom:1em;"></div>	8.3 附着歧义<div style="padding-bottom:1em;"></div>		一般使用简单的&quot;同现计数&quot;来统计即可<div style="padding-bottom:1em;"></div>		介词短语的附着歧义: 附着于动词还是名词?<div style="padding-bottom:1em;"></div>		一种自动学习方法: 先找出所有无歧义的<div style="padding-bottom:1em;"></div>	8.4 选择倾向<div style="padding-bottom:1em;"></div>		v/subject, v/object, adj/noun等搭配,在特定中心词下,常倾向于某一特定类型.<div style="padding-bottom:1em;"></div>		可对名词归类,与v和adj匹配<div style="padding-bottom:1em;"></div>		选择倾向过强的动词可能隐藏其宾语: He ate.<div style="padding-bottom:1em;"></div>	8.5 语义相似性<div style="padding-bottom:1em;"></div>		通用描述词汇的语义十分困难,因此词汇获取的最终结果往往落到语义相似性上<div style="padding-bottom:1em;"></div>		8.5.1 (名词 )转换成文档空间(或修饰词空间)的向量(同现次数),评价相似度<div style="padding-bottom:1em;"></div>		向量相似性的度量: 除余弦外还有其他特征数. 余弦较常用<div style="padding-bottom:1em;"></div>		p203 计算余弦相似性的结果. 利用对数加权方程f(x)=1+log(x)代替同现计数<div style="padding-bottom:1em;"></div>		8.5.2 有时我们的数据是概率向量,而余弦基于欧氏距离,不适合评价概率向量的相似性.<div style="padding-bottom:1em;"></div>			概率分布中相似性的度量方法: 相对熵;信息半径;L1范式(Mahatton距离,即一阶幂平均)<div style="padding-bottom:1em;"></div>	8.6 词汇获取的重要性:<div style="padding-bottom:1em;"></div>		派生词等非辞典词的存在<div style="padding-bottom:1em;"></div>#9 Markov<div style="padding-bottom:1em;"></div>=========<div style="padding-bottom:1em;"></div>	n阶Markov模型,可视为一个状态有n元的一阶Markov模型.<div style="padding-bottom:1em;"></div>	1阶Markov模型就是一个正则概率转移图, 也可看作不确定有限状态自动机.<div style="padding-bottom:1em;"></div>	Hidden Markov Model: 过程未知<div style="padding-bottom:1em;"></div>	Viterbi Algorithm: 根据起止点找最可能路径. DP<div style="padding-bottom:1em;"></div>	HMM中对原始模型的参数估计:随机选取初始值,迭代修改,可得到局部最优值<div style="padding-bottom:1em;"></div>	实现中,有些主要依靠乘法的算法(如Viterbi)常用对数来实现, 更快速且减小了浮点误差<div style="padding-bottom:1em;"></div>#10 POS tagging<div style="padding-bottom:1em;"></div>===============<div style="padding-bottom:1em;"></div>	信息源(可用数据):<div style="padding-bottom:1em;"></div>		表. 不可靠: 几乎所有名词都可作为动词-&gt;对这类信息的高级描述得到dumb tagger<div style="padding-bottom:1em;"></div>	Markov tagger. 按照POS作为状态进行转移<div style="padding-bottom:1em;"></div>		句子w[]的最佳POS序列t[]为:<div style="padding-bottom:1em;"></div>		max(t){prod(i=1~n){P(w_i | t_i )* P(t_i | t_i-1)}}<div style="padding-bottom:1em;"></div>		仍然是SSP问题<div style="padding-bottom:1em;"></div>	对未登陆词的处理: 某些可根据词形猜测,一般的给每个未登陆词一个POS分布<div style="padding-bottom:1em;"></div>	三元tagger未必更好, 对二元/三元做插值也许会好. 阶数高时注意跨越标点时的处理方法<div style="padding-bottom:1em;"></div>	无初始数据的情形(对未知语言,无语料库; 或对特殊领域),用HMM tagger, 如何初始化模型参数是关键<div style="padding-bottom:1em;"></div>	Transformation-based Learning of Tags, 即学习-重写的过程<div style="padding-bottom:1em;"></div>	不容易出现过度适应测试集的情形..?<div style="padding-bottom:1em;"></div>	其他语言中,更多的词形变化可能能为POS tagging提供更多信息. 不同语言的标注集不可比,语法不同<div style="padding-bottom:1em;"></div>	一般语料的标注准确率已能达到95%+<div style="padding-bottom:1em;"></div>	Markov的弊病: 无法处理Recursive Gramma结构!<div style="padding-bottom:1em;"></div>	**将每个单词换成语法等价的多词短语,是否可破坏多数此类tagging算法?(Markov的limit horizon)<div style="padding-bottom:1em;"></div>	The velocity rises to -&gt; The velocity of waves rises to. 难以处理,因为复数名词+单数动词少见<div style="padding-bottom:1em;"></div>#11 Probabilistic Context Free Gramma<div style="padding-bottom:1em;"></div>=====================================<div style="padding-bottom:1em;"></div>	对POS建树后,利用转移概率, 计算这种结构的概率<div style="padding-bottom:1em;"></div>	e.g.: S-&gt; NP +VP : 0.3; PP-&gt;P +NP : 1.0; NP -&gt; stars : 0.18;<div style="padding-bottom:1em;"></div>	将各节点转移概率相乘即为树概率<div style="padding-bottom:1em;"></div>	ContextFree Hypothesis: 子树的概率与其他部分无关<div style="padding-bottom:1em;"></div>	PCFG的一个欠缺:未考虑词汇的同现特征,只基于结构.因此需要与上下文知识结合.<div style="padding-bottom:1em;"></div>	PCFG使得短句子的概率更大, 但事实上WallStreetJournal的句子平均长度为23<div style="padding-bottom:1em;"></div>		解决思路不是对结果再处理,而应是找到更好的(仍基于概率的)metrics of goodness<div style="padding-bottom:1em;"></div>	PCFG中所有合法句子的概率和,并不一定是1, 实际中没什么影响.<div style="padding-bottom:1em;"></div>	计算PCFG也可构造出子结构递推, 利用类似前向-后向的方法,有内部概率-外部概率<div style="padding-bottom:1em;"></div>	内部概率是此子树的概率:选择分割点,得到子结构<div style="padding-bottom:1em;"></div>	外部概率是除此子树外的部分的概率,自上而下计算.<div style="padding-bottom:1em;"></div>	记录内部概率做DP, 即可找句子的最佳句法分析结果<div style="padding-bottom:1em;"></div>	可用EM算法训练数据, 找到最大化语料库似然性的语法.<div style="padding-bottom:1em;"></div>	Problem: 复杂度/ 局部极值/ 叶节点个数/ 参数初始化<div style="padding-bottom:1em;"></div>#12 Statistical Parsing<div style="padding-bottom:1em;"></div>=======================<div style="padding-bottom:1em;"></div>	确定句子的树状结构及POS<div style="padding-bottom:1em;"></div>	PCFG缺乏词汇化,有些词汇转移生成P +NP的概率会更高;<div style="padding-bottom:1em;"></div>	在附着歧义中更是如此,仅仅有POS只能提供很少信息<div style="padding-bottom:1em;"></div>	PCFG的概率上下文无关假设非常错误..<div style="padding-bottom:1em;"></div>	Dependency Grammar: 考虑词汇之间(语义上的)依赖关系,摈弃了庞大的结构树形式<div style="padding-bottom:1em;"></div>	评价结果: PARSEVAL度量尺度 效果粗糙<div style="padding-bottom:1em;"></div>	准确率: 有多少个标准答案中的括号; 召回率:结果有多少个括号是标准答案;交叉括号.<div style="padding-bottom:1em;"></div>	一些看不懂的Parser..<div style="padding-bottom:1em;"></div>#13 Machine Translation<div style="padding-bottom:1em;"></div>=======================<div style="padding-bottom:1em;"></div>	词的对应-&gt;词义消岐; 句法转换 -&gt;句法分析消岐; 即使同句法,仍可能有语义歧义;<div style="padding-bottom:1em;"></div>	用语义级别的媒介, 又难以设计语义表达方法.<div style="padding-bottom:1em;"></div>	文本对齐: 所谓对齐,不允许交叉,而允许组块<div style="padding-bottom:1em;"></div>	Length-Based: 假设短句对应短句,长句对应长句. 同语系时很有效<div style="padding-bottom:1em;"></div>	不同语系中会有很多1:3,3:1,3:3模式.<div style="padding-bottom:1em;"></div>	假设只存在{1:1,1:0,0:1,1:2,2:1,2:2}这几种对齐方式,用f[i][j]表示前i+前j的匹配价值,做DP<div style="padding-bottom:1em;"></div>	Length-Based只能处理clean text. 若分割标记有噪声,无法识别句子边界就会悲剧.<div style="padding-bottom:1em;"></div>	考虑词汇间的对应关系,建立双语文本映射表.压缩为bitmap后寻找一条明显轨迹.<div style="padding-bottom:1em;"></div>	Word-Based: 基本假设:分布位置相同的词语是对应的<div style="padding-bottom:1em;"></div>	方法:选取首尾固定住,中间部分交叉分析,将分布接近的词语认为对应,固定住,重复上操作.<div style="padding-bottom:1em;"></div>	只考虑实词的匹配会更好.<div style="padding-bottom:1em;"></div>	词对齐的另一个效果是可以识别(翻译可能不同的)未登陆词<div style="padding-bottom:1em;"></div>	语言模型:得到句子a的生成概率;<div style="padding-bottom:1em;"></div>	翻译模型:句子a翻译成句子b的概率-&gt;对所有可能的对齐方式求和<div style="padding-bottom:1em;"></div>		&lt;-需要知道词a翻译成词b的概率&lt;-词语分布关联性<div style="padding-bottom:1em;"></div>	词语的一对多,多对一难以解决; 词态变化,短语,长距离语法结构都需靠语言知识学习;<div style="padding-bottom:1em;"></div>	好的模型应能够分析出句子成分间的对应关系,而不是通过一对多的数据输入.<div style="padding-bottom:1em;"></div>	好的模型不应有太多的独立性假设<div style="padding-bottom:1em;"></div>#14 Clustering<div style="padding-bottom:1em;"></div>==============<div style="padding-bottom:1em;"></div>	认为上下文信息足够提供词语的相似性<div style="padding-bottom:1em;"></div>	词相似: 利用上下文的模式相似度<div style="padding-bottom:1em;"></div>	Clustering算法不需要提供训练数据,无监督.<div style="padding-bottom:1em;"></div>	hierarchical vs. flat 类别间是否有层级. flat更为简洁高效<div style="padding-bottom:1em;"></div>	soft vs. hard 每个样本是否可属于多类,属于多个类的概率分布<div style="padding-bottom:1em;"></div>	Implement: 定义出类之间的相似度函数<div style="padding-bottom:1em;"></div>	bottom-up hierarchical algorithm: 每个对象都是一个类,不断合并最相似的两个类<div style="padding-bottom:1em;"></div>	top-down: 初始只有一个类,每次将内聚程度最小的一部分元素分出去. 要求相似函数对自变量单调递减性:并集相似度低<div style="padding-bottom:1em;"></div>	单连通clustering: 用两集合最相似样本的相似度衡量集合相似度. 容易产生chaining effect.<div style="padding-bottom:1em;"></div>	算法执行过程类似MST.由相似度单调性,可类似MST快速实现<div style="padding-bottom:1em;"></div>	全连通:用最不相似样本的相似度衡量相似度. 聚的更紧密. 复杂度更高(n^3)<div style="padding-bottom:1em;"></div>	平均连通: 也即利用余弦度量相似度,合并操作可高效完成. 复杂度为n^2<div style="padding-bottom:1em;"></div>	k-means算法(hard-clustering):任选k个中心,按距中心距离聚类,用各类元素均值更新中心.<div style="padding-bottom:1em;"></div>	EM算法:按距中心距离计算各元素属于各类的概率分布<div style="padding-bottom:1em;"></div>	广义EM算法的介绍:<div style="padding-bottom:1em;"></div>#15 Information Retrieval<div style="padding-bottom:1em;"></div>=========================<div style="padding-bottom:1em;"></div>	Probability Ranking Principle:按照相关概率的降序排列文档(假设了文档间彼此不相关)<div style="padding-bottom:1em;"></div>	用n维向量空间衡量相似性. 文档在n个主题上的权重作为向量,对于normalized的向量,余弦与欧式距离给出的排序相同<div style="padding-bottom:1em;"></div>	若不进行normalized,则长文档的权重会高.<div style="padding-bottom:1em;"></div>	tf_{i,j}: w_i在d_j中出现的次数; df_i: 出现w_i的d的个数; cf_i: w_i出现的总次数<div style="padding-bottom:1em;"></div>	tfidf: w_{i,j} = (1 + log(tf_{i,j})) * log(D / df_i)<div style="padding-bottom:1em;"></div>	也有其他计算方案<div style="padding-bottom:1em;"></div>	IDF推导:<div style="padding-bottom:1em;"></div>	给定查询,我们需要按照odds of relevance: P(Rel | d) / P(NotRel | d )排序<div style="padding-bottom:1em;"></div>	对其用Bayes展开后取对数,得log P(d | Rel) - log P(d | NotRel) + log P(Rel) - log P(NotRel)<div style="padding-bottom:1em;"></div>	后两项与文档无关,排序时舍去<div style="padding-bottom:1em;"></div>	假设文档中的词独立出现,则P(d | Rel) = \prod_i{P( w_i出现 | Rel)}<div style="padding-bottom:1em;"></div>	上式对查询中的所有单词求积,取对数后,待排序函数变为:<div style="padding-bottom:1em;"></div>	O(d) = \sum_i(log P(X_i | Rel) - log P(X_i | NotRel)),其中X_i表示w_i在d中是否出现<div style="padding-bottom:1em;"></div>	设p_i = P(1 | Rel)表示w_i出现在相关文档中的概率,注意到P(X_i | Rel) = p_i^{X_i} * (1 - p_i)^{1-X_i}<div style="padding-bottom:1em;"></div>	同理定义q_i = P(1 | NotRel),函数变为:<div style="padding-bottom:1em;"></div>	O(d) = \sum_i { X_i * [log p_i/(1-p_i) + log (1-q_i)/q_i] + log (1-p_i)/(1-q_i)}<div style="padding-bottom:1em;"></div>	最后一项与X_i无关,排序时舍去. 得到O(d) = O1 + O2<div style="padding-bottom:1em;"></div>	假设p_i对所有词条是一个小常数,则O1 = \sum_i X_i * log[p_i/(1-p_i)] = c \sum_i X_i<div style="padding-bottom:1em;"></div>	假设文档中绝大多数与查询无关,则q_i = P(w_i) = df_i / N, (1-q_i)/q_i = N / df_i<div style="padding-bottom:1em;"></div>	最终O(d) =\sum_i [X_i * (c + idf_i)]<div style="padding-bottom:1em;"></div>	词条分布模型:<div style="padding-bottom:1em;"></div>	Poisson:假设单词w_i在文档中的出现次数cnt服从Poisson(L_i), L_i = cf_i / D<div style="padding-bottom:1em;"></div>	p(k, L_i) = P(cnt = k) = e^{-L_i} L_i^k / k!<div style="padding-bottom:1em;"></div>	用此分布可进一步估计df_i = N * P(cnt&gt;=1) = N (1 - p(0, L_i))<div style="padding-bottom:1em;"></div>	词义越实际,估计误差越大,因为此时词的出现不再独立<div style="padding-bottom:1em;"></div>	好处:可以用来判断虚词orz..<div style="padding-bottom:1em;"></div>	二重Poisson: 将文档分为两类:此词作为实词(重点,主题)出现的和作为虚词出现的<div style="padding-bottom:1em;"></div>	之前的方法都没有利用词语重现<div style="padding-bottom:1em;"></div>	Latent Semantic Indexing (topic model)<div style="padding-bottom:1em;"></div>	对词条-文档矩阵的SVD<div style="padding-bottom:1em;"></div>	TextTiling: 找到文档中与查询有关的段落<div style="padding-bottom:1em;"></div>	基本思想: 衡量句子的紧凑度(cohesion), 紧凑度与两边的差之和称作深度<div style="padding-bottom:1em;"></div>	紧凑度小,深度大的句子容易成为分割句<div style="padding-bottom:1em;"></div>	较好的方法:Block Comparison. 将前后句子在前后文本块里表示成向量计算距离<div style="padding-bottom:1em;"></div>#16 Text Categorization<div style="padding-bottom:1em;"></div>=======================<div style="padding-bottom:1em;"></div>	将文本抽象为向量: 将文档单词用词频计算出某种得分<div style="padding-bottom:1em;"></div>	Decision Tree: 按照一列可判定问题选择树的分支<div style="padding-bottom:1em;"></div>	训练决策树: 对于某些独特训练数据,过度训练容易出现overfitting<div style="padding-bottom:1em;"></div>	简单的stoping criterion: 当前节点的所有数据都已具有相同类别<div style="padding-bottom:1em;"></div>	Maximum information gain: 按照某个属性决策,分支信息增益最大<div style="padding-bottom:1em;"></div>	建树后剪枝以优化性能,剪枝会导致训练集上的准确度下降<div style="padding-bottom:1em;"></div>	使用留存数据(held-out data)进行验证/剪枝,是部分不使用的训练数据<div style="padding-bottom:1em;"></div>	为了更充分利用数据,可使用n-fold cross-validation,取一小部分做验证另一部分做训练,循环<div style="padding-bottom:1em;"></div>	决策树清晰易于跟踪.<div style="padding-bottom:1em;"></div>	Maximum Entropy:<div style="padding-bottom:1em;"></div>	固定一个待考察类别,选定K个单词作为base<div style="padding-bottom:1em;"></div>	构造K个特征函数f_i(x,c), x是一篇文档的向量(如取最显著的20维)<div style="padding-bottom:1em;"></div>	若c=1(文档属于目标类别的一维),且x中单词w_i的权重&gt;0,则f_i(x)=1, else =0<div style="padding-bottom:1em;"></div>	loglinear model:<div style="padding-bottom:1em;"></div>	p(x,c) =Z * \prod_(i=1~K){a_i ^{f_i(x, c)} },a_i为待训练参数,表示各个特征的权重,Z normalized<div style="padding-bottom:1em;"></div>	分类时比较p(x,1)与p(x,0)的大小<div style="padding-bottom:1em;"></div>	广义比例迭代法求最大熵模型,要求特征期望相同?<div style="padding-bottom:1em;"></div>	效果很好(96%)<div style="padding-bottom:1em;"></div>	最大熵方法提供了一个整合各个特征的良好框架<div style="padding-bottom:1em;"></div>	Perceptron:<div style="padding-bottom:1em;"></div>	用向量点积与阈值比较做决策,遇到不符合当前模型的样本,就修改此维度上的判断向量和阈值(朝梯度最大方向)<div style="padding-bottom:1em;"></div>	对于linearly separable问题,此学习一定收敛.<div style="padding-bottom:1em;"></div>	但文本分类在word-based的向量空间中不线性可分.(83%)<div style="padding-bottom:1em;"></div>	Nearest neighbor classification:<div style="padding-bottom:1em;"></div>	依赖一个好的相似度计算函数,效率低<div style="padding-bottom:1em;"></div>	特定问题下效果好.

                  <div class="col-rec-con clearfix">
                    







<div class="rec-sec">

    <span class="rec">

<a href="https://www.douban.com/accounts/register?reason=collect" class="j a_show_login lnk-sharing lnk-douban-sharing">推荐</a>
</span>
</div>

                  </div>
                <div class="pl col-time">
                  <a href="https://book.douban.com/annotation/24558993/#comments">回应</a>&nbsp;&nbsp;
                  2013-02-14 00:02
                </div>
              </div>
            </div>
          </div>
        </div>
      </li>
  </ul>
  

      
  <ul class="comments by_page"  style="display: none">
      
      <li class="ctsh clearfix" data-cid="24558993">
        <div class="ilst">
          <a href="https://www.douban.com/people/ppwwyyxx/"><img src="https://img3.doubanio.com/icon/u33367269-4.jpg" alt="Silvery" class="" /></a>
        </div>
        <div class="con">
          <div class="nlst">
            <h3>
              <div class="note-toggle rr">
                <a href="https://book.douban.com/annotation/24558993/" class="note-unfolder">展开</a>
                <a href="javascript:void(0);" class="note-folder">收起</a>
              </div>
              <a href="https://book.douban.com/annotation/24558993/" class="">第1页</a></h3>
          </div>
          <div class="clst">
            <p class="user"><a href="https://www.douban.com/people/ppwwyyxx/" class=" " title="Silvery">Silvery</a>
                (autumn leaves)
              
                <span class="allstar50" title="力荐"></span>
            </p>
            <div class="reading-note" data-cid="24558993">
              <div class="short">
                
                  <span class="">第一本带了专业方向性的书,做点小记录
前半本有价值的东西不多..
另外..豆瓣上发东西要怎么才能让格式好看点...

#1.绪
=====
	不关心是否合乎语法这一分类

	语言的自我发展:
	19c才出现kind of/sort of的程度修饰用法

	起因:a kind of adj. n. -&gt; (kind of adj.) n.

	利用词性+语法的parse,歧义太多:
	List the sales of the products produced in 1973 with the products produced in 1972
	有455种句法分..</span>
                <p class="pl">
                  <span class="">2013-02-14 00:02</span>
                  
                </p>
              </div>
              <div class="all hidden" style="display:none" >
                第一本带了专业方向性的书,做点小记录<div style="padding-bottom:1em;"></div>前半本有价值的东西不多..<div style="padding-bottom:1em;"></div>另外..豆瓣上发东西要怎么才能让格式好看点...<div style="padding-bottom:1em;"></div>#1.绪<div style="padding-bottom:1em;"></div>=====<div style="padding-bottom:1em;"></div>	不关心是否合乎语法这一分类<div style="padding-bottom:1em;"></div>	语言的自我发展:<div style="padding-bottom:1em;"></div>	19c才出现kind of/sort of的程度修饰用法<div style="padding-bottom:1em;"></div>	起因:a kind of adj. n. -&gt; (kind of adj.) n.<div style="padding-bottom:1em;"></div>	利用词性+语法的parse,歧义太多:<div style="padding-bottom:1em;"></div>	List the sales of the products produced in 1973 with the products produced in 1972<div style="padding-bottom:1em;"></div>	有455种句法分析结果(Martin)<div style="padding-bottom:1em;"></div>	增加限制与优选规则(只能手工添加)非常费力,且无法处理生动的(修辞)语言.<div style="padding-bottom:1em;"></div>	相反,统计方法自动归纳结构信息,挖掘搭配关系.<div style="padding-bottom:1em;"></div>	Garden Pathing现象:<div style="padding-bottom:1em;"></div>	The horse raced past the barn fell.<div style="padding-bottom:1em;"></div>	发现无法分析后要回溯到The horse.在口语中因为语气及停顿,不会有此问题<div style="padding-bottom:1em;"></div>	Brown语料库/Susanne语料库(free)<div style="padding-bottom:1em;"></div>	token- 总词次  type- 总词数<div style="padding-bottom:1em;"></div>	Zipf法则:大型语料库中,一个单词的词频与词频排名成反比.<div style="padding-bottom:1em;"></div>	Entropy: H(x) = -E[log(p(x))], H(X,Y) = -E[log(p(x,y))], H(Y|X) = -E[log (p(y|x))]<div style="padding-bottom:1em;"></div>	(chain rule:) H(X,Y) = H(X) + H(Y|X)<div style="padding-bottom:1em;"></div>	Mutual Information: I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)<div style="padding-bottom:1em;"></div>	已知一者对另一者不确定性的减少量<div style="padding-bottom:1em;"></div>	I(X;Y) = E[log(p(x,y) / p(x)p(y) )]<div style="padding-bottom:1em;"></div>	噪声信道模型: 通过最大化编码的entropy, 可使得噪声信道的输入与输出的互信息最<div style="padding-bottom:1em;"></div>	大化, 此时传输速率可等于信道capacity.<div style="padding-bottom:1em;"></div>	有限记忆性-&gt;k阶Markov链.<div style="padding-bottom:1em;"></div>	用一阶二阶模型估算英语的熵约为3.3-4, Shannon人工实验得到1.34<div style="padding-bottom:1em;"></div>#4.2文本<div style="padding-bottom:1em;"></div>========<div style="padding-bottom:1em;"></div>	文本程序预处理中的许多实际问题(格式与标点相关的识别困	难)<div style="padding-bottom:1em;"></div>#6 N-gram model<div style="padding-bottom:1em;"></div>===============<div style="padding-bottom:1em;"></div>	可以想象一个靠前k个词及谓语的预测会很可靠. 但谓语识别很困难.<div style="padding-bottom:1em;"></div>	trigram模型仍然是非常好的预测模型.<div style="padding-bottom:1em;"></div>	高阶n-gram模型只适合在巨量测试语料上使用.<div style="padding-bottom:1em;"></div>	如何给未登录词赋一个非0概率?<div style="padding-bottom:1em;"></div>	Laplace: 全部分子+1<div style="padding-bottom:1em;"></div>	Lidstone: +t, Jeffreys-perkes: t = 1/2时是期望似然估计<div style="padding-bottom:1em;"></div>	选一个小t以避免太多概率空间转移到未知事件.<div style="padding-bottom:1em;"></div>	6.2.3如何选取训练数据和测试数据<div style="padding-bottom:1em;"></div>	p138网站<div style="padding-bottom:1em;"></div>	利用n-gram的频率给出概率估计的各类技巧:<div style="padding-bottom:1em;"></div>	deleted estimation, deleted interpolation estimation, Good-Turing estimation<div style="padding-bottom:1em;"></div>	6.3:通过不同阶(较小)的n-gram频率来估计给定n值的n-gram概率,可以有助于数据稀疏问题<div style="padding-bottom:1em;"></div>	线性插值:P(wn|wn-2, wn-1) = a1P(wn) + a2P(wn|wn-1) + a3P(wn|wn-1, wn-2)<div style="padding-bottom:1em;"></div>	可利用EM算法(Expectation Maximization)确定最佳权值<div style="padding-bottom:1em;"></div>	Katz回退算法..<div style="padding-bottom:1em;"></div>	一般化线性插值: 系数权值是关于历史的函数<div style="padding-bottom:1em;"></div>#7 语义消岐<div style="padding-bottom:1em;"></div>===========<div style="padding-bottom:1em;"></div>	有监督学习:含语义标注数据. 无监督学习(clustering):聚类<div style="padding-bottom:1em;"></div>	以人类的标注成功率作为效果的上界,将所有词指定为其最常用语义作为其效果的下界<div style="padding-bottom:1em;"></div>	7.2有监督消岐<div style="padding-bottom:1em;"></div>	7.2.1 Bayes Classification<div style="padding-bottom:1em;"></div>	决定w的语义s:<div style="padding-bottom:1em;"></div>	s = max(s_k){P(s_k | c)} = max(s_k)P(c| s_k)P(s_k) (上下文c(context window)的概率为常量)<div style="padding-bottom:1em;"></div>	  = max(s_k){log P(c| s_k) + log P(s_k) }<div style="padding-bottom:1em;"></div>	使用Naive Bayes Assumption, 赋予其独立性,则上式继续<div style="padding-bottom:1em;"></div>	  = max(s_k){log P(s_k) + sum(v in c){log P(v | s_k) } }<div style="padding-bottom:1em;"></div>	其中的P(v|s), P(s)可从语料(必须是已标记好的)中利用MLE计算出(最好加上适当的平滑)<div style="padding-bottom:1em;"></div>	P(v_t|s) = C(v_t, s) / sum(v_i)C(v_i,s); P(s) = C(s) / C(words)<div style="padding-bottom:1em;"></div>	7.2.2 An Approach using Information Theroy<div style="padding-bottom:1em;"></div>	将w可能的语义集合P,与其对应指示器(如下文的两个单词)的可能集合Q做划分<div style="padding-bottom:1em;"></div>	P = {P1, P2, ...}, Q = {Q1, Q2, ...}<div style="padding-bottom:1em;"></div>	使得这样划分后互信息I(P, Q)最大,重复迭代二者可保证解的存在<div style="padding-bottom:1em;"></div>	w = {做} P = {take, make}, Q = {measure, note, decision}<div style="padding-bottom:1em;"></div>	7.3Dict-Based Disambiguation<div style="padding-bottom:1em;"></div>	7.3.1语义定义<div style="padding-bottom:1em;"></div>	判断w的语义时,挑选与w的附近词定义相似性最大的定义(看词语重复)<div style="padding-bottom:1em;"></div>	7.3.2类义<div style="padding-bottom:1em;"></div>	利用包含语义范畴信息的词典,每个词有若干个tag<div style="padding-bottom:1em;"></div>	在运行中可以对遇到的词添加新的tag(Classification)<div style="padding-bottom:1em;"></div>	类义的范畴有时与语义无法对应匹配,因此用于语义消岐效果不好.<div style="padding-bottom:1em;"></div>	7.3.3基于第二语言语料库. 利用目标词的上下文的翻译的上下文寻找结果.<div style="padding-bottom:1em;"></div>	7.3.4文本语义:一词在整篇文本中更可能取同一语义.<div style="padding-bottom:1em;"></div>	7.4无监督消岐<div style="padding-bottom:1em;"></div>	随机初始化P(v| s_k), 根据EM算法重新估计P(v| s_k), 使得模型整体的似然值保持增长<div style="padding-bottom:1em;"></div>#8 词汇获取<div style="padding-bottom:1em;"></div>===========<div style="padding-bottom:1em;"></div>	8.1 关于假设检验,拒真受伪<div style="padding-bottom:1em;"></div>	8.2 动词子范畴帮助句法分析<div style="padding-bottom:1em;"></div>		动词可引导什么样的结构. tell sb sth; find sb adj;<div style="padding-bottom:1em;"></div>		利用这样的范畴作出假设,检验,自我修复<div style="padding-bottom:1em;"></div>	8.3 附着歧义<div style="padding-bottom:1em;"></div>		一般使用简单的&quot;同现计数&quot;来统计即可<div style="padding-bottom:1em;"></div>		介词短语的附着歧义: 附着于动词还是名词?<div style="padding-bottom:1em;"></div>		一种自动学习方法: 先找出所有无歧义的<div style="padding-bottom:1em;"></div>	8.4 选择倾向<div style="padding-bottom:1em;"></div>		v/subject, v/object, adj/noun等搭配,在特定中心词下,常倾向于某一特定类型.<div style="padding-bottom:1em;"></div>		可对名词归类,与v和adj匹配<div style="padding-bottom:1em;"></div>		选择倾向过强的动词可能隐藏其宾语: He ate.<div style="padding-bottom:1em;"></div>	8.5 语义相似性<div style="padding-bottom:1em;"></div>		通用描述词汇的语义十分困难,因此词汇获取的最终结果往往落到语义相似性上<div style="padding-bottom:1em;"></div>		8.5.1 (名词 )转换成文档空间(或修饰词空间)的向量(同现次数),评价相似度<div style="padding-bottom:1em;"></div>		向量相似性的度量: 除余弦外还有其他特征数. 余弦较常用<div style="padding-bottom:1em;"></div>		p203 计算余弦相似性的结果. 利用对数加权方程f(x)=1+log(x)代替同现计数<div style="padding-bottom:1em;"></div>		8.5.2 有时我们的数据是概率向量,而余弦基于欧氏距离,不适合评价概率向量的相似性.<div style="padding-bottom:1em;"></div>			概率分布中相似性的度量方法: 相对熵;信息半径;L1范式(Mahatton距离,即一阶幂平均)<div style="padding-bottom:1em;"></div>	8.6 词汇获取的重要性:<div style="padding-bottom:1em;"></div>		派生词等非辞典词的存在<div style="padding-bottom:1em;"></div>#9 Markov<div style="padding-bottom:1em;"></div>=========<div style="padding-bottom:1em;"></div>	n阶Markov模型,可视为一个状态有n元的一阶Markov模型.<div style="padding-bottom:1em;"></div>	1阶Markov模型就是一个正则概率转移图, 也可看作不确定有限状态自动机.<div style="padding-bottom:1em;"></div>	Hidden Markov Model: 过程未知<div style="padding-bottom:1em;"></div>	Viterbi Algorithm: 根据起止点找最可能路径. DP<div style="padding-bottom:1em;"></div>	HMM中对原始模型的参数估计:随机选取初始值,迭代修改,可得到局部最优值<div style="padding-bottom:1em;"></div>	实现中,有些主要依靠乘法的算法(如Viterbi)常用对数来实现, 更快速且减小了浮点误差<div style="padding-bottom:1em;"></div>#10 POS tagging<div style="padding-bottom:1em;"></div>===============<div style="padding-bottom:1em;"></div>	信息源(可用数据):<div style="padding-bottom:1em;"></div>		表. 不可靠: 几乎所有名词都可作为动词-&gt;对这类信息的高级描述得到dumb tagger<div style="padding-bottom:1em;"></div>	Markov tagger. 按照POS作为状态进行转移<div style="padding-bottom:1em;"></div>		句子w[]的最佳POS序列t[]为:<div style="padding-bottom:1em;"></div>		max(t){prod(i=1~n){P(w_i | t_i )* P(t_i | t_i-1)}}<div style="padding-bottom:1em;"></div>		仍然是SSP问题<div style="padding-bottom:1em;"></div>	对未登陆词的处理: 某些可根据词形猜测,一般的给每个未登陆词一个POS分布<div style="padding-bottom:1em;"></div>	三元tagger未必更好, 对二元/三元做插值也许会好. 阶数高时注意跨越标点时的处理方法<div style="padding-bottom:1em;"></div>	无初始数据的情形(对未知语言,无语料库; 或对特殊领域),用HMM tagger, 如何初始化模型参数是关键<div style="padding-bottom:1em;"></div>	Transformation-based Learning of Tags, 即学习-重写的过程<div style="padding-bottom:1em;"></div>	不容易出现过度适应测试集的情形..?<div style="padding-bottom:1em;"></div>	其他语言中,更多的词形变化可能能为POS tagging提供更多信息. 不同语言的标注集不可比,语法不同<div style="padding-bottom:1em;"></div>	一般语料的标注准确率已能达到95%+<div style="padding-bottom:1em;"></div>	Markov的弊病: 无法处理Recursive Gramma结构!<div style="padding-bottom:1em;"></div>	**将每个单词换成语法等价的多词短语,是否可破坏多数此类tagging算法?(Markov的limit horizon)<div style="padding-bottom:1em;"></div>	The velocity rises to -&gt; The velocity of waves rises to. 难以处理,因为复数名词+单数动词少见<div style="padding-bottom:1em;"></div>#11 Probabilistic Context Free Gramma<div style="padding-bottom:1em;"></div>=====================================<div style="padding-bottom:1em;"></div>	对POS建树后,利用转移概率, 计算这种结构的概率<div style="padding-bottom:1em;"></div>	e.g.: S-&gt; NP +VP : 0.3; PP-&gt;P +NP : 1.0; NP -&gt; stars : 0.18;<div style="padding-bottom:1em;"></div>	将各节点转移概率相乘即为树概率<div style="padding-bottom:1em;"></div>	ContextFree Hypothesis: 子树的概率与其他部分无关<div style="padding-bottom:1em;"></div>	PCFG的一个欠缺:未考虑词汇的同现特征,只基于结构.因此需要与上下文知识结合.<div style="padding-bottom:1em;"></div>	PCFG使得短句子的概率更大, 但事实上WallStreetJournal的句子平均长度为23<div style="padding-bottom:1em;"></div>		解决思路不是对结果再处理,而应是找到更好的(仍基于概率的)metrics of goodness<div style="padding-bottom:1em;"></div>	PCFG中所有合法句子的概率和,并不一定是1, 实际中没什么影响.<div style="padding-bottom:1em;"></div>	计算PCFG也可构造出子结构递推, 利用类似前向-后向的方法,有内部概率-外部概率<div style="padding-bottom:1em;"></div>	内部概率是此子树的概率:选择分割点,得到子结构<div style="padding-bottom:1em;"></div>	外部概率是除此子树外的部分的概率,自上而下计算.<div style="padding-bottom:1em;"></div>	记录内部概率做DP, 即可找句子的最佳句法分析结果<div style="padding-bottom:1em;"></div>	可用EM算法训练数据, 找到最大化语料库似然性的语法.<div style="padding-bottom:1em;"></div>	Problem: 复杂度/ 局部极值/ 叶节点个数/ 参数初始化<div style="padding-bottom:1em;"></div>#12 Statistical Parsing<div style="padding-bottom:1em;"></div>=======================<div style="padding-bottom:1em;"></div>	确定句子的树状结构及POS<div style="padding-bottom:1em;"></div>	PCFG缺乏词汇化,有些词汇转移生成P +NP的概率会更高;<div style="padding-bottom:1em;"></div>	在附着歧义中更是如此,仅仅有POS只能提供很少信息<div style="padding-bottom:1em;"></div>	PCFG的概率上下文无关假设非常错误..<div style="padding-bottom:1em;"></div>	Dependency Grammar: 考虑词汇之间(语义上的)依赖关系,摈弃了庞大的结构树形式<div style="padding-bottom:1em;"></div>	评价结果: PARSEVAL度量尺度 效果粗糙<div style="padding-bottom:1em;"></div>	准确率: 有多少个标准答案中的括号; 召回率:结果有多少个括号是标准答案;交叉括号.<div style="padding-bottom:1em;"></div>	一些看不懂的Parser..<div style="padding-bottom:1em;"></div>#13 Machine Translation<div style="padding-bottom:1em;"></div>=======================<div style="padding-bottom:1em;"></div>	词的对应-&gt;词义消岐; 句法转换 -&gt;句法分析消岐; 即使同句法,仍可能有语义歧义;<div style="padding-bottom:1em;"></div>	用语义级别的媒介, 又难以设计语义表达方法.<div style="padding-bottom:1em;"></div>	文本对齐: 所谓对齐,不允许交叉,而允许组块<div style="padding-bottom:1em;"></div>	Length-Based: 假设短句对应短句,长句对应长句. 同语系时很有效<div style="padding-bottom:1em;"></div>	不同语系中会有很多1:3,3:1,3:3模式.<div style="padding-bottom:1em;"></div>	假设只存在{1:1,1:0,0:1,1:2,2:1,2:2}这几种对齐方式,用f[i][j]表示前i+前j的匹配价值,做DP<div style="padding-bottom:1em;"></div>	Length-Based只能处理clean text. 若分割标记有噪声,无法识别句子边界就会悲剧.<div style="padding-bottom:1em;"></div>	考虑词汇间的对应关系,建立双语文本映射表.压缩为bitmap后寻找一条明显轨迹.<div style="padding-bottom:1em;"></div>	Word-Based: 基本假设:分布位置相同的词语是对应的<div style="padding-bottom:1em;"></div>	方法:选取首尾固定住,中间部分交叉分析,将分布接近的词语认为对应,固定住,重复上操作.<div style="padding-bottom:1em;"></div>	只考虑实词的匹配会更好.<div style="padding-bottom:1em;"></div>	词对齐的另一个效果是可以识别(翻译可能不同的)未登陆词<div style="padding-bottom:1em;"></div>	语言模型:得到句子a的生成概率;<div style="padding-bottom:1em;"></div>	翻译模型:句子a翻译成句子b的概率-&gt;对所有可能的对齐方式求和<div style="padding-bottom:1em;"></div>		&lt;-需要知道词a翻译成词b的概率&lt;-词语分布关联性<div style="padding-bottom:1em;"></div>	词语的一对多,多对一难以解决; 词态变化,短语,长距离语法结构都需靠语言知识学习;<div style="padding-bottom:1em;"></div>	好的模型应能够分析出句子成分间的对应关系,而不是通过一对多的数据输入.<div style="padding-bottom:1em;"></div>	好的模型不应有太多的独立性假设<div style="padding-bottom:1em;"></div>#14 Clustering<div style="padding-bottom:1em;"></div>==============<div style="padding-bottom:1em;"></div>	认为上下文信息足够提供词语的相似性<div style="padding-bottom:1em;"></div>	词相似: 利用上下文的模式相似度<div style="padding-bottom:1em;"></div>	Clustering算法不需要提供训练数据,无监督.<div style="padding-bottom:1em;"></div>	hierarchical vs. flat 类别间是否有层级. flat更为简洁高效<div style="padding-bottom:1em;"></div>	soft vs. hard 每个样本是否可属于多类,属于多个类的概率分布<div style="padding-bottom:1em;"></div>	Implement: 定义出类之间的相似度函数<div style="padding-bottom:1em;"></div>	bottom-up hierarchical algorithm: 每个对象都是一个类,不断合并最相似的两个类<div style="padding-bottom:1em;"></div>	top-down: 初始只有一个类,每次将内聚程度最小的一部分元素分出去. 要求相似函数对自变量单调递减性:并集相似度低<div style="padding-bottom:1em;"></div>	单连通clustering: 用两集合最相似样本的相似度衡量集合相似度. 容易产生chaining effect.<div style="padding-bottom:1em;"></div>	算法执行过程类似MST.由相似度单调性,可类似MST快速实现<div style="padding-bottom:1em;"></div>	全连通:用最不相似样本的相似度衡量相似度. 聚的更紧密. 复杂度更高(n^3)<div style="padding-bottom:1em;"></div>	平均连通: 也即利用余弦度量相似度,合并操作可高效完成. 复杂度为n^2<div style="padding-bottom:1em;"></div>	k-means算法(hard-clustering):任选k个中心,按距中心距离聚类,用各类元素均值更新中心.<div style="padding-bottom:1em;"></div>	EM算法:按距中心距离计算各元素属于各类的概率分布<div style="padding-bottom:1em;"></div>	广义EM算法的介绍:<div style="padding-bottom:1em;"></div>#15 Information Retrieval<div style="padding-bottom:1em;"></div>=========================<div style="padding-bottom:1em;"></div>	Probability Ranking Principle:按照相关概率的降序排列文档(假设了文档间彼此不相关)<div style="padding-bottom:1em;"></div>	用n维向量空间衡量相似性. 文档在n个主题上的权重作为向量,对于normalized的向量,余弦与欧式距离给出的排序相同<div style="padding-bottom:1em;"></div>	若不进行normalized,则长文档的权重会高.<div style="padding-bottom:1em;"></div>	tf_{i,j}: w_i在d_j中出现的次数; df_i: 出现w_i的d的个数; cf_i: w_i出现的总次数<div style="padding-bottom:1em;"></div>	tfidf: w_{i,j} = (1 + log(tf_{i,j})) * log(D / df_i)<div style="padding-bottom:1em;"></div>	也有其他计算方案<div style="padding-bottom:1em;"></div>	IDF推导:<div style="padding-bottom:1em;"></div>	给定查询,我们需要按照odds of relevance: P(Rel | d) / P(NotRel | d )排序<div style="padding-bottom:1em;"></div>	对其用Bayes展开后取对数,得log P(d | Rel) - log P(d | NotRel) + log P(Rel) - log P(NotRel)<div style="padding-bottom:1em;"></div>	后两项与文档无关,排序时舍去<div style="padding-bottom:1em;"></div>	假设文档中的词独立出现,则P(d | Rel) = \prod_i{P( w_i出现 | Rel)}<div style="padding-bottom:1em;"></div>	上式对查询中的所有单词求积,取对数后,待排序函数变为:<div style="padding-bottom:1em;"></div>	O(d) = \sum_i(log P(X_i | Rel) - log P(X_i | NotRel)),其中X_i表示w_i在d中是否出现<div style="padding-bottom:1em;"></div>	设p_i = P(1 | Rel)表示w_i出现在相关文档中的概率,注意到P(X_i | Rel) = p_i^{X_i} * (1 - p_i)^{1-X_i}<div style="padding-bottom:1em;"></div>	同理定义q_i = P(1 | NotRel),函数变为:<div style="padding-bottom:1em;"></div>	O(d) = \sum_i { X_i * [log p_i/(1-p_i) + log (1-q_i)/q_i] + log (1-p_i)/(1-q_i)}<div style="padding-bottom:1em;"></div>	最后一项与X_i无关,排序时舍去. 得到O(d) = O1 + O2<div style="padding-bottom:1em;"></div>	假设p_i对所有词条是一个小常数,则O1 = \sum_i X_i * log[p_i/(1-p_i)] = c \sum_i X_i<div style="padding-bottom:1em;"></div>	假设文档中绝大多数与查询无关,则q_i = P(w_i) = df_i / N, (1-q_i)/q_i = N / df_i<div style="padding-bottom:1em;"></div>	最终O(d) =\sum_i [X_i * (c + idf_i)]<div style="padding-bottom:1em;"></div>	词条分布模型:<div style="padding-bottom:1em;"></div>	Poisson:假设单词w_i在文档中的出现次数cnt服从Poisson(L_i), L_i = cf_i / D<div style="padding-bottom:1em;"></div>	p(k, L_i) = P(cnt = k) = e^{-L_i} L_i^k / k!<div style="padding-bottom:1em;"></div>	用此分布可进一步估计df_i = N * P(cnt&gt;=1) = N (1 - p(0, L_i))<div style="padding-bottom:1em;"></div>	词义越实际,估计误差越大,因为此时词的出现不再独立<div style="padding-bottom:1em;"></div>	好处:可以用来判断虚词orz..<div style="padding-bottom:1em;"></div>	二重Poisson: 将文档分为两类:此词作为实词(重点,主题)出现的和作为虚词出现的<div style="padding-bottom:1em;"></div>	之前的方法都没有利用词语重现<div style="padding-bottom:1em;"></div>	Latent Semantic Indexing (topic model)<div style="padding-bottom:1em;"></div>	对词条-文档矩阵的SVD<div style="padding-bottom:1em;"></div>	TextTiling: 找到文档中与查询有关的段落<div style="padding-bottom:1em;"></div>	基本思想: 衡量句子的紧凑度(cohesion), 紧凑度与两边的差之和称作深度<div style="padding-bottom:1em;"></div>	紧凑度小,深度大的句子容易成为分割句<div style="padding-bottom:1em;"></div>	较好的方法:Block Comparison. 将前后句子在前后文本块里表示成向量计算距离<div style="padding-bottom:1em;"></div>#16 Text Categorization<div style="padding-bottom:1em;"></div>=======================<div style="padding-bottom:1em;"></div>	将文本抽象为向量: 将文档单词用词频计算出某种得分<div style="padding-bottom:1em;"></div>	Decision Tree: 按照一列可判定问题选择树的分支<div style="padding-bottom:1em;"></div>	训练决策树: 对于某些独特训练数据,过度训练容易出现overfitting<div style="padding-bottom:1em;"></div>	简单的stoping criterion: 当前节点的所有数据都已具有相同类别<div style="padding-bottom:1em;"></div>	Maximum information gain: 按照某个属性决策,分支信息增益最大<div style="padding-bottom:1em;"></div>	建树后剪枝以优化性能,剪枝会导致训练集上的准确度下降<div style="padding-bottom:1em;"></div>	使用留存数据(held-out data)进行验证/剪枝,是部分不使用的训练数据<div style="padding-bottom:1em;"></div>	为了更充分利用数据,可使用n-fold cross-validation,取一小部分做验证另一部分做训练,循环<div style="padding-bottom:1em;"></div>	决策树清晰易于跟踪.<div style="padding-bottom:1em;"></div>	Maximum Entropy:<div style="padding-bottom:1em;"></div>	固定一个待考察类别,选定K个单词作为base<div style="padding-bottom:1em;"></div>	构造K个特征函数f_i(x,c), x是一篇文档的向量(如取最显著的20维)<div style="padding-bottom:1em;"></div>	若c=1(文档属于目标类别的一维),且x中单词w_i的权重&gt;0,则f_i(x)=1, else =0<div style="padding-bottom:1em;"></div>	loglinear model:<div style="padding-bottom:1em;"></div>	p(x,c) =Z * \prod_(i=1~K){a_i ^{f_i(x, c)} },a_i为待训练参数,表示各个特征的权重,Z normalized<div style="padding-bottom:1em;"></div>	分类时比较p(x,1)与p(x,0)的大小<div style="padding-bottom:1em;"></div>	广义比例迭代法求最大熵模型,要求特征期望相同?<div style="padding-bottom:1em;"></div>	效果很好(96%)<div style="padding-bottom:1em;"></div>	最大熵方法提供了一个整合各个特征的良好框架<div style="padding-bottom:1em;"></div>	Perceptron:<div style="padding-bottom:1em;"></div>	用向量点积与阈值比较做决策,遇到不符合当前模型的样本,就修改此维度上的判断向量和阈值(朝梯度最大方向)<div style="padding-bottom:1em;"></div>	对于linearly separable问题,此学习一定收敛.<div style="padding-bottom:1em;"></div>	但文本分类在word-based的向量空间中不线性可分.(83%)<div style="padding-bottom:1em;"></div>	Nearest neighbor classification:<div style="padding-bottom:1em;"></div>	依赖一个好的相似度计算函数,效率低<div style="padding-bottom:1em;"></div>	特定问题下效果好.

                  <div class="col-rec-con clearfix">
                    







<div class="rec-sec">

    <span class="rec">

<a href="https://www.douban.com/accounts/register?reason=collect" class="j a_show_login lnk-sharing lnk-douban-sharing">推荐</a>
</span>
</div>

                  </div>
                <div class="pl col-time">
                  <a href="https://book.douban.com/annotation/24558993/#comments">回应</a>&nbsp;&nbsp;
                  2013-02-14 00:02
                </div>
              </div>
            </div>
          </div>
        </div>
      </li>
      
      <li class="ctsh clearfix" data-cid="34999032">
        <div class="ilst">
          <a href="https://www.douban.com/people/frankwang/"><img src="https://img3.doubanio.com/icon/u1558440-45.jpg" alt="黠之大者" class="" /></a>
        </div>
        <div class="con">
          <div class="nlst">
            <h3>
              <div class="note-toggle rr">
                <a href="https://book.douban.com/annotation/34999032/" class="note-unfolder">展开</a>
                <a href="javascript:void(0);" class="note-folder">收起</a>
              </div>
              <a href="https://book.douban.com/annotation/34999032/" class="">第121页</a></h3>
          </div>
          <div class="clst">
            <p class="user"><a href="https://www.douban.com/people/frankwang/" class=" " title="黠之大者">黠之大者</a>
                (I am serious with python)
              
            </p>
            <div class="reading-note" data-cid="34999032">
              <div class="short">
                
                  <span class="">区分n-gram数量B与词汇量V

这本书当中依然有很多错误，译者也助长了错误。在第六章 语言模型部分，作者详细定义了各种概念，但是对于B的翻译不够好：训练实例的类别量，其实就是模型的参数数量或者n-gram的数量。围绕这个概念问题，出了一系列错误。

第一个错误表现在127页的译者注释，译者注意到manning公布的勘误表，注意到：“训练语料有273266个词形，B应该是273266，。。。译者注”。

这里的B应该是V，对于二元语法..</span>
                <p class="pl">
                  <span class="">2015-05-04 12:05</span>
                  
                </p>
              </div>
              <div class="all hidden" style="display:none" >
                区分n-gram数量B与词汇量V<div style="padding-bottom:1em;"></div>这本书当中依然有很多错误，译者也助长了错误。在第六章 语言模型部分，作者详细定义了各种概念，但是对于B的翻译不够好：训练实例的类别量，其实就是模型的参数数量或者n-gram的数量。围绕这个概念问题，出了一系列错误。<div style="padding-bottom:1em;"></div>第一个错误表现在127页的译者注释，译者注意到manning公布的勘误表，注意到：“训练语料有273266个词形，B应该是273266，。。。译者注”。<div style="padding-bottom:1em;"></div>这里的B应该是V，对于二元语法模型B = V^2。<div style="padding-bottom:1em;"></div>第二个错误是128页：“这个训练语料库共有14585个词形。所以对于新的条件概率p(not|was)，新的估计是(608 + 0.5)/(9404 + 14589*0.5)”。这里也跟着错，应该是：<div style="padding-bottom:1em;"></div>(608 + 0.5)/(9404 + 14589^2*0.5)<div style="padding-bottom:1em;"></div>当然这里是由于原作者错误，译者不察觉。相应地，表格6.5里的ELE估计都是错的，原文结论说折扣掉一半也完全错误。<div style="padding-bottom:1em;"></div>结论是第六章出现的一系列错误作者难辞其咎。译者也未能指出错误。<div style="padding-bottom:1em;"></div><a rel="nofollow" href="http://nlp.stanford.edu/fsnlp/errata.html" target="_blank">http://nlp.stanford.edu/fsnlp/errata.html</a><div style="padding-bottom:1em;"></div>page 196, line -13: Change &quot;This will be V^{n-1}&quot; to &quot;This will be V&quot;, given the following major clarification: In Section 6.1, the number of 'bins' is used to refer to the number of possible values of the classificatory feature vectors, while (unfortunately) from Section 6.2 on, with this change, the term 'bins' and the letter B is used to refer to the number of values of the target feature. This is V for prediction of the next word, but V^n for predicting the frequency of n-grams. (Thanks to Tibor Kiss &lt;tibor .... linguistics.ruhr-uni-bochum.de&gt;<div style="padding-bottom:1em;"></div>page 202-203: While the whole corpus had 400,653 word types, the training corpus had only 273,266 word types. This smaller number should have been used as B in the calculation of a Laplace's law estimate of table 6.4 (whereas actually 400,653 was used). The result of this change is that f_{Lap}(0) = 0.000295, and then 99.96% of the probability mass is given to previously unseen bigrams (!). In such a model, note that we have used a (demonstrably wrong) closed vocabulary assumption, so despite this huge mass being given to unseen bigrams, none is being given to potential bigrams using vocabulary items outside the training set vocabulary (OOV = out of vocabulary items). (Thanks to Steve Renals &lt;s.renals .... dcs.shef.ac.uk&gt; and Gary Cottrell &lt;gary .... cs.ucsd.edu&gt;<div style="padding-bottom:1em;"></div>page 205, line 2-3: Correction: here it is said that there are 14589 word types, but the number given elsewhere in the chapter (and the actual number found on rechecking the data file) is 14585. Clarification: Here we directly smooth the conditional distributions, so there are only |V| = 14585 values for the bigram conditional distribution added into the denominator during smoothing, whereas on pp. 202-203, we were estimating bigram probabilities, and there are |V|^2 different bigrams. (Thanks to Hidetosi Sirai &lt;sirai .... sccs.chukyo-u.ac.jp&gt;, Mark Lewellen &lt;lewellen .... erols.com&gt;, and Gary Cottrell &lt;gary .... cs.ucsd.edu&gt;
                  <div class="col-rec-con clearfix">
                    







<div class="rec-sec">

    <span class="rec">

<a href="https://www.douban.com/accounts/register?reason=collect" class="j a_show_login lnk-sharing lnk-douban-sharing">推荐</a>
</span>
</div>

                  </div>
                <div class="pl col-time">
                  <a href="https://book.douban.com/annotation/34999032/#comments">回应</a>&nbsp;&nbsp;
                  2015-05-04 12:05
                </div>
              </div>
            </div>
          </div>
        </div>
      </li>
  </ul>
  

      
  <ul class="comments by_time"  style="display: none">
      
      <li class="ctsh clearfix" data-cid="34999032">
        <div class="ilst">
          <a href="https://www.douban.com/people/frankwang/"><img src="https://img3.doubanio.com/icon/u1558440-45.jpg" alt="黠之大者" class="" /></a>
        </div>
        <div class="con">
          <div class="nlst">
            <h3>
              <div class="note-toggle rr">
                <a href="https://book.douban.com/annotation/34999032/" class="note-unfolder">展开</a>
                <a href="javascript:void(0);" class="note-folder">收起</a>
              </div>
              <a href="https://book.douban.com/annotation/34999032/" class="">第121页</a></h3>
          </div>
          <div class="clst">
            <p class="user"><a href="https://www.douban.com/people/frankwang/" class=" " title="黠之大者">黠之大者</a>
                (I am serious with python)
              
            </p>
            <div class="reading-note" data-cid="34999032">
              <div class="short">
                
                  <span class="">区分n-gram数量B与词汇量V

这本书当中依然有很多错误，译者也助长了错误。在第六章 语言模型部分，作者详细定义了各种概念，但是对于B的翻译不够好：训练实例的类别量，其实就是模型的参数数量或者n-gram的数量。围绕这个概念问题，出了一系列错误。

第一个错误表现在127页的译者注释，译者注意到manning公布的勘误表，注意到：“训练语料有273266个词形，B应该是273266，。。。译者注”。

这里的B应该是V，对于二元语法..</span>
                <p class="pl">
                  <span class="">2015-05-04 12:05</span>
                  
                </p>
              </div>
              <div class="all hidden" style="display:none" >
                区分n-gram数量B与词汇量V<div style="padding-bottom:1em;"></div>这本书当中依然有很多错误，译者也助长了错误。在第六章 语言模型部分，作者详细定义了各种概念，但是对于B的翻译不够好：训练实例的类别量，其实就是模型的参数数量或者n-gram的数量。围绕这个概念问题，出了一系列错误。<div style="padding-bottom:1em;"></div>第一个错误表现在127页的译者注释，译者注意到manning公布的勘误表，注意到：“训练语料有273266个词形，B应该是273266，。。。译者注”。<div style="padding-bottom:1em;"></div>这里的B应该是V，对于二元语法模型B = V^2。<div style="padding-bottom:1em;"></div>第二个错误是128页：“这个训练语料库共有14585个词形。所以对于新的条件概率p(not|was)，新的估计是(608 + 0.5)/(9404 + 14589*0.5)”。这里也跟着错，应该是：<div style="padding-bottom:1em;"></div>(608 + 0.5)/(9404 + 14589^2*0.5)<div style="padding-bottom:1em;"></div>当然这里是由于原作者错误，译者不察觉。相应地，表格6.5里的ELE估计都是错的，原文结论说折扣掉一半也完全错误。<div style="padding-bottom:1em;"></div>结论是第六章出现的一系列错误作者难辞其咎。译者也未能指出错误。<div style="padding-bottom:1em;"></div><a rel="nofollow" href="http://nlp.stanford.edu/fsnlp/errata.html" target="_blank">http://nlp.stanford.edu/fsnlp/errata.html</a><div style="padding-bottom:1em;"></div>page 196, line -13: Change &quot;This will be V^{n-1}&quot; to &quot;This will be V&quot;, given the following major clarification: In Section 6.1, the number of 'bins' is used to refer to the number of possible values of the classificatory feature vectors, while (unfortunately) from Section 6.2 on, with this change, the term 'bins' and the letter B is used to refer to the number of values of the target feature. This is V for prediction of the next word, but V^n for predicting the frequency of n-grams. (Thanks to Tibor Kiss &lt;tibor .... linguistics.ruhr-uni-bochum.de&gt;<div style="padding-bottom:1em;"></div>page 202-203: While the whole corpus had 400,653 word types, the training corpus had only 273,266 word types. This smaller number should have been used as B in the calculation of a Laplace's law estimate of table 6.4 (whereas actually 400,653 was used). The result of this change is that f_{Lap}(0) = 0.000295, and then 99.96% of the probability mass is given to previously unseen bigrams (!). In such a model, note that we have used a (demonstrably wrong) closed vocabulary assumption, so despite this huge mass being given to unseen bigrams, none is being given to potential bigrams using vocabulary items outside the training set vocabulary (OOV = out of vocabulary items). (Thanks to Steve Renals &lt;s.renals .... dcs.shef.ac.uk&gt; and Gary Cottrell &lt;gary .... cs.ucsd.edu&gt;<div style="padding-bottom:1em;"></div>page 205, line 2-3: Correction: here it is said that there are 14589 word types, but the number given elsewhere in the chapter (and the actual number found on rechecking the data file) is 14585. Clarification: Here we directly smooth the conditional distributions, so there are only |V| = 14585 values for the bigram conditional distribution added into the denominator during smoothing, whereas on pp. 202-203, we were estimating bigram probabilities, and there are |V|^2 different bigrams. (Thanks to Hidetosi Sirai &lt;sirai .... sccs.chukyo-u.ac.jp&gt;, Mark Lewellen &lt;lewellen .... erols.com&gt;, and Gary Cottrell &lt;gary .... cs.ucsd.edu&gt;
                  <div class="col-rec-con clearfix">
                    







<div class="rec-sec">

    <span class="rec">

<a href="https://www.douban.com/accounts/register?reason=collect" class="j a_show_login lnk-sharing lnk-douban-sharing">推荐</a>
</span>
</div>

                  </div>
                <div class="pl col-time">
                  <a href="https://book.douban.com/annotation/34999032/#comments">回应</a>&nbsp;&nbsp;
                  2015-05-04 12:05
                </div>
              </div>
            </div>
          </div>
        </div>
      </li>
      
      <li class="ctsh clearfix" data-cid="24558993">
        <div class="ilst">
          <a href="https://www.douban.com/people/ppwwyyxx/"><img src="https://img3.doubanio.com/icon/u33367269-4.jpg" alt="Silvery" class="" /></a>
        </div>
        <div class="con">
          <div class="nlst">
            <h3>
              <div class="note-toggle rr">
                <a href="https://book.douban.com/annotation/24558993/" class="note-unfolder">展开</a>
                <a href="javascript:void(0);" class="note-folder">收起</a>
              </div>
              <a href="https://book.douban.com/annotation/24558993/" class="">第1页</a></h3>
          </div>
          <div class="clst">
            <p class="user"><a href="https://www.douban.com/people/ppwwyyxx/" class=" " title="Silvery">Silvery</a>
                (autumn leaves)
              
                <span class="allstar50" title="力荐"></span>
            </p>
            <div class="reading-note" data-cid="24558993">
              <div class="short">
                
                  <span class="">第一本带了专业方向性的书,做点小记录
前半本有价值的东西不多..
另外..豆瓣上发东西要怎么才能让格式好看点...

#1.绪
=====
	不关心是否合乎语法这一分类

	语言的自我发展:
	19c才出现kind of/sort of的程度修饰用法

	起因:a kind of adj. n. -&gt; (kind of adj.) n.

	利用词性+语法的parse,歧义太多:
	List the sales of the products produced in 1973 with the products produced in 1972
	有455种句法分..</span>
                <p class="pl">
                  <span class="">2013-02-14 00:02</span>
                  
                </p>
              </div>
              <div class="all hidden" style="display:none" >
                第一本带了专业方向性的书,做点小记录<div style="padding-bottom:1em;"></div>前半本有价值的东西不多..<div style="padding-bottom:1em;"></div>另外..豆瓣上发东西要怎么才能让格式好看点...<div style="padding-bottom:1em;"></div>#1.绪<div style="padding-bottom:1em;"></div>=====<div style="padding-bottom:1em;"></div>	不关心是否合乎语法这一分类<div style="padding-bottom:1em;"></div>	语言的自我发展:<div style="padding-bottom:1em;"></div>	19c才出现kind of/sort of的程度修饰用法<div style="padding-bottom:1em;"></div>	起因:a kind of adj. n. -&gt; (kind of adj.) n.<div style="padding-bottom:1em;"></div>	利用词性+语法的parse,歧义太多:<div style="padding-bottom:1em;"></div>	List the sales of the products produced in 1973 with the products produced in 1972<div style="padding-bottom:1em;"></div>	有455种句法分析结果(Martin)<div style="padding-bottom:1em;"></div>	增加限制与优选规则(只能手工添加)非常费力,且无法处理生动的(修辞)语言.<div style="padding-bottom:1em;"></div>	相反,统计方法自动归纳结构信息,挖掘搭配关系.<div style="padding-bottom:1em;"></div>	Garden Pathing现象:<div style="padding-bottom:1em;"></div>	The horse raced past the barn fell.<div style="padding-bottom:1em;"></div>	发现无法分析后要回溯到The horse.在口语中因为语气及停顿,不会有此问题<div style="padding-bottom:1em;"></div>	Brown语料库/Susanne语料库(free)<div style="padding-bottom:1em;"></div>	token- 总词次  type- 总词数<div style="padding-bottom:1em;"></div>	Zipf法则:大型语料库中,一个单词的词频与词频排名成反比.<div style="padding-bottom:1em;"></div>	Entropy: H(x) = -E[log(p(x))], H(X,Y) = -E[log(p(x,y))], H(Y|X) = -E[log (p(y|x))]<div style="padding-bottom:1em;"></div>	(chain rule:) H(X,Y) = H(X) + H(Y|X)<div style="padding-bottom:1em;"></div>	Mutual Information: I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)<div style="padding-bottom:1em;"></div>	已知一者对另一者不确定性的减少量<div style="padding-bottom:1em;"></div>	I(X;Y) = E[log(p(x,y) / p(x)p(y) )]<div style="padding-bottom:1em;"></div>	噪声信道模型: 通过最大化编码的entropy, 可使得噪声信道的输入与输出的互信息最<div style="padding-bottom:1em;"></div>	大化, 此时传输速率可等于信道capacity.<div style="padding-bottom:1em;"></div>	有限记忆性-&gt;k阶Markov链.<div style="padding-bottom:1em;"></div>	用一阶二阶模型估算英语的熵约为3.3-4, Shannon人工实验得到1.34<div style="padding-bottom:1em;"></div>#4.2文本<div style="padding-bottom:1em;"></div>========<div style="padding-bottom:1em;"></div>	文本程序预处理中的许多实际问题(格式与标点相关的识别困	难)<div style="padding-bottom:1em;"></div>#6 N-gram model<div style="padding-bottom:1em;"></div>===============<div style="padding-bottom:1em;"></div>	可以想象一个靠前k个词及谓语的预测会很可靠. 但谓语识别很困难.<div style="padding-bottom:1em;"></div>	trigram模型仍然是非常好的预测模型.<div style="padding-bottom:1em;"></div>	高阶n-gram模型只适合在巨量测试语料上使用.<div style="padding-bottom:1em;"></div>	如何给未登录词赋一个非0概率?<div style="padding-bottom:1em;"></div>	Laplace: 全部分子+1<div style="padding-bottom:1em;"></div>	Lidstone: +t, Jeffreys-perkes: t = 1/2时是期望似然估计<div style="padding-bottom:1em;"></div>	选一个小t以避免太多概率空间转移到未知事件.<div style="padding-bottom:1em;"></div>	6.2.3如何选取训练数据和测试数据<div style="padding-bottom:1em;"></div>	p138网站<div style="padding-bottom:1em;"></div>	利用n-gram的频率给出概率估计的各类技巧:<div style="padding-bottom:1em;"></div>	deleted estimation, deleted interpolation estimation, Good-Turing estimation<div style="padding-bottom:1em;"></div>	6.3:通过不同阶(较小)的n-gram频率来估计给定n值的n-gram概率,可以有助于数据稀疏问题<div style="padding-bottom:1em;"></div>	线性插值:P(wn|wn-2, wn-1) = a1P(wn) + a2P(wn|wn-1) + a3P(wn|wn-1, wn-2)<div style="padding-bottom:1em;"></div>	可利用EM算法(Expectation Maximization)确定最佳权值<div style="padding-bottom:1em;"></div>	Katz回退算法..<div style="padding-bottom:1em;"></div>	一般化线性插值: 系数权值是关于历史的函数<div style="padding-bottom:1em;"></div>#7 语义消岐<div style="padding-bottom:1em;"></div>===========<div style="padding-bottom:1em;"></div>	有监督学习:含语义标注数据. 无监督学习(clustering):聚类<div style="padding-bottom:1em;"></div>	以人类的标注成功率作为效果的上界,将所有词指定为其最常用语义作为其效果的下界<div style="padding-bottom:1em;"></div>	7.2有监督消岐<div style="padding-bottom:1em;"></div>	7.2.1 Bayes Classification<div style="padding-bottom:1em;"></div>	决定w的语义s:<div style="padding-bottom:1em;"></div>	s = max(s_k){P(s_k | c)} = max(s_k)P(c| s_k)P(s_k) (上下文c(context window)的概率为常量)<div style="padding-bottom:1em;"></div>	  = max(s_k){log P(c| s_k) + log P(s_k) }<div style="padding-bottom:1em;"></div>	使用Naive Bayes Assumption, 赋予其独立性,则上式继续<div style="padding-bottom:1em;"></div>	  = max(s_k){log P(s_k) + sum(v in c){log P(v | s_k) } }<div style="padding-bottom:1em;"></div>	其中的P(v|s), P(s)可从语料(必须是已标记好的)中利用MLE计算出(最好加上适当的平滑)<div style="padding-bottom:1em;"></div>	P(v_t|s) = C(v_t, s) / sum(v_i)C(v_i,s); P(s) = C(s) / C(words)<div style="padding-bottom:1em;"></div>	7.2.2 An Approach using Information Theroy<div style="padding-bottom:1em;"></div>	将w可能的语义集合P,与其对应指示器(如下文的两个单词)的可能集合Q做划分<div style="padding-bottom:1em;"></div>	P = {P1, P2, ...}, Q = {Q1, Q2, ...}<div style="padding-bottom:1em;"></div>	使得这样划分后互信息I(P, Q)最大,重复迭代二者可保证解的存在<div style="padding-bottom:1em;"></div>	w = {做} P = {take, make}, Q = {measure, note, decision}<div style="padding-bottom:1em;"></div>	7.3Dict-Based Disambiguation<div style="padding-bottom:1em;"></div>	7.3.1语义定义<div style="padding-bottom:1em;"></div>	判断w的语义时,挑选与w的附近词定义相似性最大的定义(看词语重复)<div style="padding-bottom:1em;"></div>	7.3.2类义<div style="padding-bottom:1em;"></div>	利用包含语义范畴信息的词典,每个词有若干个tag<div style="padding-bottom:1em;"></div>	在运行中可以对遇到的词添加新的tag(Classification)<div style="padding-bottom:1em;"></div>	类义的范畴有时与语义无法对应匹配,因此用于语义消岐效果不好.<div style="padding-bottom:1em;"></div>	7.3.3基于第二语言语料库. 利用目标词的上下文的翻译的上下文寻找结果.<div style="padding-bottom:1em;"></div>	7.3.4文本语义:一词在整篇文本中更可能取同一语义.<div style="padding-bottom:1em;"></div>	7.4无监督消岐<div style="padding-bottom:1em;"></div>	随机初始化P(v| s_k), 根据EM算法重新估计P(v| s_k), 使得模型整体的似然值保持增长<div style="padding-bottom:1em;"></div>#8 词汇获取<div style="padding-bottom:1em;"></div>===========<div style="padding-bottom:1em;"></div>	8.1 关于假设检验,拒真受伪<div style="padding-bottom:1em;"></div>	8.2 动词子范畴帮助句法分析<div style="padding-bottom:1em;"></div>		动词可引导什么样的结构. tell sb sth; find sb adj;<div style="padding-bottom:1em;"></div>		利用这样的范畴作出假设,检验,自我修复<div style="padding-bottom:1em;"></div>	8.3 附着歧义<div style="padding-bottom:1em;"></div>		一般使用简单的&quot;同现计数&quot;来统计即可<div style="padding-bottom:1em;"></div>		介词短语的附着歧义: 附着于动词还是名词?<div style="padding-bottom:1em;"></div>		一种自动学习方法: 先找出所有无歧义的<div style="padding-bottom:1em;"></div>	8.4 选择倾向<div style="padding-bottom:1em;"></div>		v/subject, v/object, adj/noun等搭配,在特定中心词下,常倾向于某一特定类型.<div style="padding-bottom:1em;"></div>		可对名词归类,与v和adj匹配<div style="padding-bottom:1em;"></div>		选择倾向过强的动词可能隐藏其宾语: He ate.<div style="padding-bottom:1em;"></div>	8.5 语义相似性<div style="padding-bottom:1em;"></div>		通用描述词汇的语义十分困难,因此词汇获取的最终结果往往落到语义相似性上<div style="padding-bottom:1em;"></div>		8.5.1 (名词 )转换成文档空间(或修饰词空间)的向量(同现次数),评价相似度<div style="padding-bottom:1em;"></div>		向量相似性的度量: 除余弦外还有其他特征数. 余弦较常用<div style="padding-bottom:1em;"></div>		p203 计算余弦相似性的结果. 利用对数加权方程f(x)=1+log(x)代替同现计数<div style="padding-bottom:1em;"></div>		8.5.2 有时我们的数据是概率向量,而余弦基于欧氏距离,不适合评价概率向量的相似性.<div style="padding-bottom:1em;"></div>			概率分布中相似性的度量方法: 相对熵;信息半径;L1范式(Mahatton距离,即一阶幂平均)<div style="padding-bottom:1em;"></div>	8.6 词汇获取的重要性:<div style="padding-bottom:1em;"></div>		派生词等非辞典词的存在<div style="padding-bottom:1em;"></div>#9 Markov<div style="padding-bottom:1em;"></div>=========<div style="padding-bottom:1em;"></div>	n阶Markov模型,可视为一个状态有n元的一阶Markov模型.<div style="padding-bottom:1em;"></div>	1阶Markov模型就是一个正则概率转移图, 也可看作不确定有限状态自动机.<div style="padding-bottom:1em;"></div>	Hidden Markov Model: 过程未知<div style="padding-bottom:1em;"></div>	Viterbi Algorithm: 根据起止点找最可能路径. DP<div style="padding-bottom:1em;"></div>	HMM中对原始模型的参数估计:随机选取初始值,迭代修改,可得到局部最优值<div style="padding-bottom:1em;"></div>	实现中,有些主要依靠乘法的算法(如Viterbi)常用对数来实现, 更快速且减小了浮点误差<div style="padding-bottom:1em;"></div>#10 POS tagging<div style="padding-bottom:1em;"></div>===============<div style="padding-bottom:1em;"></div>	信息源(可用数据):<div style="padding-bottom:1em;"></div>		表. 不可靠: 几乎所有名词都可作为动词-&gt;对这类信息的高级描述得到dumb tagger<div style="padding-bottom:1em;"></div>	Markov tagger. 按照POS作为状态进行转移<div style="padding-bottom:1em;"></div>		句子w[]的最佳POS序列t[]为:<div style="padding-bottom:1em;"></div>		max(t){prod(i=1~n){P(w_i | t_i )* P(t_i | t_i-1)}}<div style="padding-bottom:1em;"></div>		仍然是SSP问题<div style="padding-bottom:1em;"></div>	对未登陆词的处理: 某些可根据词形猜测,一般的给每个未登陆词一个POS分布<div style="padding-bottom:1em;"></div>	三元tagger未必更好, 对二元/三元做插值也许会好. 阶数高时注意跨越标点时的处理方法<div style="padding-bottom:1em;"></div>	无初始数据的情形(对未知语言,无语料库; 或对特殊领域),用HMM tagger, 如何初始化模型参数是关键<div style="padding-bottom:1em;"></div>	Transformation-based Learning of Tags, 即学习-重写的过程<div style="padding-bottom:1em;"></div>	不容易出现过度适应测试集的情形..?<div style="padding-bottom:1em;"></div>	其他语言中,更多的词形变化可能能为POS tagging提供更多信息. 不同语言的标注集不可比,语法不同<div style="padding-bottom:1em;"></div>	一般语料的标注准确率已能达到95%+<div style="padding-bottom:1em;"></div>	Markov的弊病: 无法处理Recursive Gramma结构!<div style="padding-bottom:1em;"></div>	**将每个单词换成语法等价的多词短语,是否可破坏多数此类tagging算法?(Markov的limit horizon)<div style="padding-bottom:1em;"></div>	The velocity rises to -&gt; The velocity of waves rises to. 难以处理,因为复数名词+单数动词少见<div style="padding-bottom:1em;"></div>#11 Probabilistic Context Free Gramma<div style="padding-bottom:1em;"></div>=====================================<div style="padding-bottom:1em;"></div>	对POS建树后,利用转移概率, 计算这种结构的概率<div style="padding-bottom:1em;"></div>	e.g.: S-&gt; NP +VP : 0.3; PP-&gt;P +NP : 1.0; NP -&gt; stars : 0.18;<div style="padding-bottom:1em;"></div>	将各节点转移概率相乘即为树概率<div style="padding-bottom:1em;"></div>	ContextFree Hypothesis: 子树的概率与其他部分无关<div style="padding-bottom:1em;"></div>	PCFG的一个欠缺:未考虑词汇的同现特征,只基于结构.因此需要与上下文知识结合.<div style="padding-bottom:1em;"></div>	PCFG使得短句子的概率更大, 但事实上WallStreetJournal的句子平均长度为23<div style="padding-bottom:1em;"></div>		解决思路不是对结果再处理,而应是找到更好的(仍基于概率的)metrics of goodness<div style="padding-bottom:1em;"></div>	PCFG中所有合法句子的概率和,并不一定是1, 实际中没什么影响.<div style="padding-bottom:1em;"></div>	计算PCFG也可构造出子结构递推, 利用类似前向-后向的方法,有内部概率-外部概率<div style="padding-bottom:1em;"></div>	内部概率是此子树的概率:选择分割点,得到子结构<div style="padding-bottom:1em;"></div>	外部概率是除此子树外的部分的概率,自上而下计算.<div style="padding-bottom:1em;"></div>	记录内部概率做DP, 即可找句子的最佳句法分析结果<div style="padding-bottom:1em;"></div>	可用EM算法训练数据, 找到最大化语料库似然性的语法.<div style="padding-bottom:1em;"></div>	Problem: 复杂度/ 局部极值/ 叶节点个数/ 参数初始化<div style="padding-bottom:1em;"></div>#12 Statistical Parsing<div style="padding-bottom:1em;"></div>=======================<div style="padding-bottom:1em;"></div>	确定句子的树状结构及POS<div style="padding-bottom:1em;"></div>	PCFG缺乏词汇化,有些词汇转移生成P +NP的概率会更高;<div style="padding-bottom:1em;"></div>	在附着歧义中更是如此,仅仅有POS只能提供很少信息<div style="padding-bottom:1em;"></div>	PCFG的概率上下文无关假设非常错误..<div style="padding-bottom:1em;"></div>	Dependency Grammar: 考虑词汇之间(语义上的)依赖关系,摈弃了庞大的结构树形式<div style="padding-bottom:1em;"></div>	评价结果: PARSEVAL度量尺度 效果粗糙<div style="padding-bottom:1em;"></div>	准确率: 有多少个标准答案中的括号; 召回率:结果有多少个括号是标准答案;交叉括号.<div style="padding-bottom:1em;"></div>	一些看不懂的Parser..<div style="padding-bottom:1em;"></div>#13 Machine Translation<div style="padding-bottom:1em;"></div>=======================<div style="padding-bottom:1em;"></div>	词的对应-&gt;词义消岐; 句法转换 -&gt;句法分析消岐; 即使同句法,仍可能有语义歧义;<div style="padding-bottom:1em;"></div>	用语义级别的媒介, 又难以设计语义表达方法.<div style="padding-bottom:1em;"></div>	文本对齐: 所谓对齐,不允许交叉,而允许组块<div style="padding-bottom:1em;"></div>	Length-Based: 假设短句对应短句,长句对应长句. 同语系时很有效<div style="padding-bottom:1em;"></div>	不同语系中会有很多1:3,3:1,3:3模式.<div style="padding-bottom:1em;"></div>	假设只存在{1:1,1:0,0:1,1:2,2:1,2:2}这几种对齐方式,用f[i][j]表示前i+前j的匹配价值,做DP<div style="padding-bottom:1em;"></div>	Length-Based只能处理clean text. 若分割标记有噪声,无法识别句子边界就会悲剧.<div style="padding-bottom:1em;"></div>	考虑词汇间的对应关系,建立双语文本映射表.压缩为bitmap后寻找一条明显轨迹.<div style="padding-bottom:1em;"></div>	Word-Based: 基本假设:分布位置相同的词语是对应的<div style="padding-bottom:1em;"></div>	方法:选取首尾固定住,中间部分交叉分析,将分布接近的词语认为对应,固定住,重复上操作.<div style="padding-bottom:1em;"></div>	只考虑实词的匹配会更好.<div style="padding-bottom:1em;"></div>	词对齐的另一个效果是可以识别(翻译可能不同的)未登陆词<div style="padding-bottom:1em;"></div>	语言模型:得到句子a的生成概率;<div style="padding-bottom:1em;"></div>	翻译模型:句子a翻译成句子b的概率-&gt;对所有可能的对齐方式求和<div style="padding-bottom:1em;"></div>		&lt;-需要知道词a翻译成词b的概率&lt;-词语分布关联性<div style="padding-bottom:1em;"></div>	词语的一对多,多对一难以解决; 词态变化,短语,长距离语法结构都需靠语言知识学习;<div style="padding-bottom:1em;"></div>	好的模型应能够分析出句子成分间的对应关系,而不是通过一对多的数据输入.<div style="padding-bottom:1em;"></div>	好的模型不应有太多的独立性假设<div style="padding-bottom:1em;"></div>#14 Clustering<div style="padding-bottom:1em;"></div>==============<div style="padding-bottom:1em;"></div>	认为上下文信息足够提供词语的相似性<div style="padding-bottom:1em;"></div>	词相似: 利用上下文的模式相似度<div style="padding-bottom:1em;"></div>	Clustering算法不需要提供训练数据,无监督.<div style="padding-bottom:1em;"></div>	hierarchical vs. flat 类别间是否有层级. flat更为简洁高效<div style="padding-bottom:1em;"></div>	soft vs. hard 每个样本是否可属于多类,属于多个类的概率分布<div style="padding-bottom:1em;"></div>	Implement: 定义出类之间的相似度函数<div style="padding-bottom:1em;"></div>	bottom-up hierarchical algorithm: 每个对象都是一个类,不断合并最相似的两个类<div style="padding-bottom:1em;"></div>	top-down: 初始只有一个类,每次将内聚程度最小的一部分元素分出去. 要求相似函数对自变量单调递减性:并集相似度低<div style="padding-bottom:1em;"></div>	单连通clustering: 用两集合最相似样本的相似度衡量集合相似度. 容易产生chaining effect.<div style="padding-bottom:1em;"></div>	算法执行过程类似MST.由相似度单调性,可类似MST快速实现<div style="padding-bottom:1em;"></div>	全连通:用最不相似样本的相似度衡量相似度. 聚的更紧密. 复杂度更高(n^3)<div style="padding-bottom:1em;"></div>	平均连通: 也即利用余弦度量相似度,合并操作可高效完成. 复杂度为n^2<div style="padding-bottom:1em;"></div>	k-means算法(hard-clustering):任选k个中心,按距中心距离聚类,用各类元素均值更新中心.<div style="padding-bottom:1em;"></div>	EM算法:按距中心距离计算各元素属于各类的概率分布<div style="padding-bottom:1em;"></div>	广义EM算法的介绍:<div style="padding-bottom:1em;"></div>#15 Information Retrieval<div style="padding-bottom:1em;"></div>=========================<div style="padding-bottom:1em;"></div>	Probability Ranking Principle:按照相关概率的降序排列文档(假设了文档间彼此不相关)<div style="padding-bottom:1em;"></div>	用n维向量空间衡量相似性. 文档在n个主题上的权重作为向量,对于normalized的向量,余弦与欧式距离给出的排序相同<div style="padding-bottom:1em;"></div>	若不进行normalized,则长文档的权重会高.<div style="padding-bottom:1em;"></div>	tf_{i,j}: w_i在d_j中出现的次数; df_i: 出现w_i的d的个数; cf_i: w_i出现的总次数<div style="padding-bottom:1em;"></div>	tfidf: w_{i,j} = (1 + log(tf_{i,j})) * log(D / df_i)<div style="padding-bottom:1em;"></div>	也有其他计算方案<div style="padding-bottom:1em;"></div>	IDF推导:<div style="padding-bottom:1em;"></div>	给定查询,我们需要按照odds of relevance: P(Rel | d) / P(NotRel | d )排序<div style="padding-bottom:1em;"></div>	对其用Bayes展开后取对数,得log P(d | Rel) - log P(d | NotRel) + log P(Rel) - log P(NotRel)<div style="padding-bottom:1em;"></div>	后两项与文档无关,排序时舍去<div style="padding-bottom:1em;"></div>	假设文档中的词独立出现,则P(d | Rel) = \prod_i{P( w_i出现 | Rel)}<div style="padding-bottom:1em;"></div>	上式对查询中的所有单词求积,取对数后,待排序函数变为:<div style="padding-bottom:1em;"></div>	O(d) = \sum_i(log P(X_i | Rel) - log P(X_i | NotRel)),其中X_i表示w_i在d中是否出现<div style="padding-bottom:1em;"></div>	设p_i = P(1 | Rel)表示w_i出现在相关文档中的概率,注意到P(X_i | Rel) = p_i^{X_i} * (1 - p_i)^{1-X_i}<div style="padding-bottom:1em;"></div>	同理定义q_i = P(1 | NotRel),函数变为:<div style="padding-bottom:1em;"></div>	O(d) = \sum_i { X_i * [log p_i/(1-p_i) + log (1-q_i)/q_i] + log (1-p_i)/(1-q_i)}<div style="padding-bottom:1em;"></div>	最后一项与X_i无关,排序时舍去. 得到O(d) = O1 + O2<div style="padding-bottom:1em;"></div>	假设p_i对所有词条是一个小常数,则O1 = \sum_i X_i * log[p_i/(1-p_i)] = c \sum_i X_i<div style="padding-bottom:1em;"></div>	假设文档中绝大多数与查询无关,则q_i = P(w_i) = df_i / N, (1-q_i)/q_i = N / df_i<div style="padding-bottom:1em;"></div>	最终O(d) =\sum_i [X_i * (c + idf_i)]<div style="padding-bottom:1em;"></div>	词条分布模型:<div style="padding-bottom:1em;"></div>	Poisson:假设单词w_i在文档中的出现次数cnt服从Poisson(L_i), L_i = cf_i / D<div style="padding-bottom:1em;"></div>	p(k, L_i) = P(cnt = k) = e^{-L_i} L_i^k / k!<div style="padding-bottom:1em;"></div>	用此分布可进一步估计df_i = N * P(cnt&gt;=1) = N (1 - p(0, L_i))<div style="padding-bottom:1em;"></div>	词义越实际,估计误差越大,因为此时词的出现不再独立<div style="padding-bottom:1em;"></div>	好处:可以用来判断虚词orz..<div style="padding-bottom:1em;"></div>	二重Poisson: 将文档分为两类:此词作为实词(重点,主题)出现的和作为虚词出现的<div style="padding-bottom:1em;"></div>	之前的方法都没有利用词语重现<div style="padding-bottom:1em;"></div>	Latent Semantic Indexing (topic model)<div style="padding-bottom:1em;"></div>	对词条-文档矩阵的SVD<div style="padding-bottom:1em;"></div>	TextTiling: 找到文档中与查询有关的段落<div style="padding-bottom:1em;"></div>	基本思想: 衡量句子的紧凑度(cohesion), 紧凑度与两边的差之和称作深度<div style="padding-bottom:1em;"></div>	紧凑度小,深度大的句子容易成为分割句<div style="padding-bottom:1em;"></div>	较好的方法:Block Comparison. 将前后句子在前后文本块里表示成向量计算距离<div style="padding-bottom:1em;"></div>#16 Text Categorization<div style="padding-bottom:1em;"></div>=======================<div style="padding-bottom:1em;"></div>	将文本抽象为向量: 将文档单词用词频计算出某种得分<div style="padding-bottom:1em;"></div>	Decision Tree: 按照一列可判定问题选择树的分支<div style="padding-bottom:1em;"></div>	训练决策树: 对于某些独特训练数据,过度训练容易出现overfitting<div style="padding-bottom:1em;"></div>	简单的stoping criterion: 当前节点的所有数据都已具有相同类别<div style="padding-bottom:1em;"></div>	Maximum information gain: 按照某个属性决策,分支信息增益最大<div style="padding-bottom:1em;"></div>	建树后剪枝以优化性能,剪枝会导致训练集上的准确度下降<div style="padding-bottom:1em;"></div>	使用留存数据(held-out data)进行验证/剪枝,是部分不使用的训练数据<div style="padding-bottom:1em;"></div>	为了更充分利用数据,可使用n-fold cross-validation,取一小部分做验证另一部分做训练,循环<div style="padding-bottom:1em;"></div>	决策树清晰易于跟踪.<div style="padding-bottom:1em;"></div>	Maximum Entropy:<div style="padding-bottom:1em;"></div>	固定一个待考察类别,选定K个单词作为base<div style="padding-bottom:1em;"></div>	构造K个特征函数f_i(x,c), x是一篇文档的向量(如取最显著的20维)<div style="padding-bottom:1em;"></div>	若c=1(文档属于目标类别的一维),且x中单词w_i的权重&gt;0,则f_i(x)=1, else =0<div style="padding-bottom:1em;"></div>	loglinear model:<div style="padding-bottom:1em;"></div>	p(x,c) =Z * \prod_(i=1~K){a_i ^{f_i(x, c)} },a_i为待训练参数,表示各个特征的权重,Z normalized<div style="padding-bottom:1em;"></div>	分类时比较p(x,1)与p(x,0)的大小<div style="padding-bottom:1em;"></div>	广义比例迭代法求最大熵模型,要求特征期望相同?<div style="padding-bottom:1em;"></div>	效果很好(96%)<div style="padding-bottom:1em;"></div>	最大熵方法提供了一个整合各个特征的良好框架<div style="padding-bottom:1em;"></div>	Perceptron:<div style="padding-bottom:1em;"></div>	用向量点积与阈值比较做决策,遇到不符合当前模型的样本,就修改此维度上的判断向量和阈值(朝梯度最大方向)<div style="padding-bottom:1em;"></div>	对于linearly separable问题,此学习一定收敛.<div style="padding-bottom:1em;"></div>	但文本分类在word-based的向量空间中不线性可分.(83%)<div style="padding-bottom:1em;"></div>	Nearest neighbor classification:<div style="padding-bottom:1em;"></div>	依赖一个好的相似度计算函数,效率低<div style="padding-bottom:1em;"></div>	特定问题下效果好.

                  <div class="col-rec-con clearfix">
                    







<div class="rec-sec">

    <span class="rec">

<a href="https://www.douban.com/accounts/register?reason=collect" class="j a_show_login lnk-sharing lnk-douban-sharing">推荐</a>
</span>
</div>

                  </div>
                <div class="pl col-time">
                  <a href="https://book.douban.com/annotation/24558993/#comments">回应</a>&nbsp;&nbsp;
                  2013-02-14 00:02
                </div>
              </div>
            </div>
          </div>
        </div>
      </li>
  </ul>
  

    </div>

</div>



<script type="text/javascript">
  $(document).ready(function(){
    var TEMPL_ADD_COL = '<a href="" id="n-{NOTE_ID}" class="colbutt ll add-col"><span>收藏</span></a>',
      TEMPL_DEL_COL = '<span class="pl">已收藏 &gt;<a href="" id="n-{NOTE_ID}" class="del-col">取消收藏</a></span>';

    $('body').delegate('.add-col', 'click', function(e){
      e.preventDefault();
      var nnid = $(this).attr('id').split('-')[1],
        bn_add = $(this);
      $.post_withck("/j/book/annotation_collect",{nid:nnid},function(){
        var a = $(TEMPL_DEL_COL.replace('{NOTE_ID}', nnid));
        bn_add.before(a);
        bn_add.remove();
      })
    });

    $('body').delegate('.del-col', 'click', function(e){
      e.preventDefault();
      var nnid = $(this).attr('id').split('-')[1],
        bn_del = $(this).parent();
      $.post_withck("/j/book/annotation_uncollect", {nid: nnid}, function() {
        var a = $(TEMPL_ADD_COL.replace('{NOTE_ID}', nnid));
        bn_del.before(a);
        bn_del.remove();
      })
    });

    $("pre.source").each(function(){
      var cn = $(this).attr('class').split(' ');
      l = cn[1];
      s = 'rand01';
      n = cn[2];
      $(this).snippet(n,{style: s, showNum: l});
    });

    var annotationMod = $('.reading-notes .bd')
      , annotationTabs = annotationMod.find('.inline-tabs li')
      , annotationTabLinks = annotationTabs.find('a')
      , annotationTabContents = annotationMod.find('ul.comments');

    annotationTabLinks.click(function(e){
      e.preventDefault();
      var el = $(this)
        , kind = el.attr('id');

      annotationTabs.removeClass('on');
      el.parent().addClass('on');
      annotationTabContents.hide();
      annotationTabContents.filter('.' + kind).show();
    });
  });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	jax: ["input/TeX", "output/HTML-CSS"],
    extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js","TeX/noUndefined.js"],
    tex2jax: {
		inlineMath: [ ["($", "$)"], ['\\(','\\)'] ],
		displayMath: [ ["($$","$$)"], ['\\[','\\]']],
		skipTags: ["script","noscript","style","textarea"],
		processEscapes: true,
		processEnvironments: true,
		preview: "TeX"
	},
	showProcessingMessages: false
  });
</script>


  






<div id="db-discussion-section" class="indent ugc-mod">




        
  

  <h2>
    <span class="">论坛</span>
      ······

  </h2>



        
<table class="olt"><tr><td><td><td><td></tr>
        
        <tr class="">
            <td class="pl"><a href="https://book.douban.com/subject/1224802/discussion/50131001/" title="猎兔自然语言处理开发培训" class="">猎兔自然语言处理开发培训</a></td>
            <td class="pl"><span class="">来自</span><a class="" href="https://www.douban.com/people/26975181/">lgjut</a></td>
            <td class="pl"><span class=""></span></td>
            <td class="pl"><span class="">2012-11-15</span></td>
        </tr>
        
        <tr class="">
            <td class="pl"><a href="https://book.douban.com/subject/1224802/discussion/1299893/" title="俺滴教授写滴，是非常8错" class="">俺滴教授写滴，是非常8错</a></td>
            <td class="pl"><span class="">来自</span><a class="" href="https://www.douban.com/people/ricedemouse/">ricedemouse</a></td>
            <td class="pl"><span class="">2 回应</span></td>
            <td class="pl"><span class="">2008-09-28</span></td>
        </tr>
</table>




</div>




</div>
<script type="text/javascript">
$(function(){if($.browser.msie && $.browser.version == 6.0){
    var maxWidth = parseInt($('#info').css('max-width'));
    if($('#info').width() > maxWidth)
        $('#info').width(maxWidth)
}});
</script>
</div>
      <div class="aside">
        
  
  






  <div id="dale_book_subject_top_right"></div>

  






<style type="text/css" media="screen">
  .add2cartContainer{overflow:hidden;vertical-align:bottom;line-height:1}.add2cartContainer .add2cart{margin-right:0;display:inline-block}#buyinfo .bs{margin:0}#buyinfo li{padding:10px 0;position:relative;line-height:1;border-bottom:1px solid #eaeaea}#buyinfo li a:hover{background-image:none !important}#buyinfo li a:hover .buylink-price{background:#37a}#buyinfo li .publish,#buyinfo li .other-activity{margin-top:5px}#buyinfo li .publish a,#buyinfo li .other-activity a{color:#999}#buyinfo li .publish a:hover,#buyinfo li .other-activity a:hover{color:#37a;background:none;opacity:0.5;filter:alpha(opacity=50)}#buyinfo li .buylink-price{position:absolute;right:90px;text-align:right}#buyinfo .more-info{color:#aaa;margin:6px 0 -2px 0}#buyinfo .more-ebooks{padding:10px 0;color:#37a;cursor:pointer}#buyinfo .price-page{border-bottom:0;padding:15px 0 0}#buyinfo .saved-price{display:none;margin-left:5px}#buyinfo .cart-tip{float:right;padding-right:5px}#buyinfo #buyinfo-ebook{margin-bottom:15px}#buyinfo #buyinfo-ebook .buylink-price{display:inline}#buyinfo #buyinfo-ebook li.no-border{border:0}#buyinfo-printed{margin-bottom:15px}#buyinfo-printed.no-border{border-bottom:0}#buyinfo-printed .more-ebooks{line-height:1;padding:10px 0;color:#37a;cursor:pointer;padding:10px 0 0}#buyinfo-report{display:none}#buyinfo-report .lnk-close-report{float:right;margin-top:-30px;line-height:14px}#buyinfo-report .item{margin-bottom:10px}#buyinfo-report .item input{vertical-align:text-bottom;*vertical-align:middle}#buyinfo-report .item label{margin:0 5px 0 2px}#buyinfo-report .item-submit .bn-flat{margin-right:10px}#buyinfo-report .item-price input{width:220px;border:1px solid #ccc;padding:4px}#buyinfo-report form{margin:5px 0 10px}#bi-report-btn{float:right;margin:2px 0;line-height:14px;display:none}.bi-vendor-report{color:#aaa}.bi-vendor-report-form{display:none;color:#111;margin:0 5px;line-height:25px}.gray_ad{padding:30px 20px 25px 20px;background:#f6f6f1}.gray_ad h2{margin-bottom:6px;font-size:15px}.gray_ad .ebook-tag{margin-top:5px;color:#999;font-size:12px}.bs.more-after{margin-bottom:0px}@media (-webkit-min-device-pixel-ratio: 2), (min-resolution: 192dpi){#buyinfo li a:hover{background-image:url(https://img3.doubanio.com/f/book/fc4ff7f0a3a7f452f06d586540284b9738f2fe87/pics/book/cart/icon-brown@2x.png);background-size:16px 12px}}#intervenor-buyinfo .bs{margin:0}#intervenor-buyinfo li{position:relative;border-bottom:1px solid #eaeaea;padding:10px 0;line-height:1}#intervenor-buyinfo li .basic-info{color:#494949;font-size:14px;line-height:18px}#intervenor-buyinfo li a:hover .comment{color:#f67;opacity:0.75;filter:alpha(opacity=75)}#intervenor-buyinfo li a:hover .buy-btn{background:#fff;border:1px solid #e97e7e;border-radius:2px;color:#e97e7e}#intervenor-buyinfo li a:hover .buylink-price{background:#37a}#intervenor-buyinfo li .buylink-price{position:absolute;right:90px;text-align:right}#intervenor-buyinfo li .publish,#intervenor-buyinfo li .other-activity{margin-top:5px}#intervenor-buyinfo li .publish a,#intervenor-buyinfo li .other-activity a{color:#999}#intervenor-buyinfo li .publish a:hover,#intervenor-buyinfo li .other-activity a:hover{color:#37a;background:none;opacity:0.5;filter:alpha(opacity=50)}#intervenor-buyinfo .jd-buy-icon{float:left;margin-right:3px}#intervenor-buyinfo .buy-btn{float:right;position:absolute;right:10px;bottom:3px;color:#9c9c9c;padding:0 12px;border:1px solid transparent}#intervenor-buyinfo .comment{color:#FF8C9C;margin:6px 0 -2px 0}#intervenor-buyinfo .price-page a{display:inline-block;line-height:16px !important}#intervenor-buyinfo .price-page{border-bottom:0;padding:15px 0 0}#intervenor-buyinfo .saved-price{display:none;margin-left:5px}#intervenor-buyinfo .cart-tip{float:right;padding-right:5px}#intervenor-buyinfo #buyinfo-ebook{margin-bottom:15px}#intervenor-buyinfo #buyinfo-ebook .buylink-price{display:inline}#intervenor-buyinfo #buyinfo-ebook li.no-border{border:0}#buyinfo-printed .presale-indicator{margin:0;color:#999;text-indent:0;background:none}

</style>

      <div class="gray_ad" id="buyinfo">
      <div id="buyinfo-printed" class="no-border">
        
  

  <h2>
    <span class="">其他版本有售</span>
      ······

  </h2>


        <ul class="bs noline">
            
              <li class="">
                  
                  <a target="_blank" href="https://book.douban.com/link2/?pre=0&amp;vendor=joyo&amp;srcpage=subject&amp;price=124600&amp;pos=1&amp;url=http%3A%2F%2Fwww.amazon.cn%2Fmn%2FdetailApp%2Fref%3Dasc_df_02621336012890177%2F%3Fasin%3D0262133601%26tag%3Ddouban-23%26creative%3D2384%26creativeASIN%3D0262133601%26linkCode%3Ddf0&amp;srcsubj=1224802&amp;type=bkbuy&amp;subject=1776634">
                    亚马逊
                  </a>
                  <a class="buylink-price" target="_blank" href="https://book.douban.com/link2/?pre=0&amp;vendor=joyo&amp;srcpage=subject&amp;price=124600&amp;pos=1&amp;url=http%3A%2F%2Fwww.amazon.cn%2Fmn%2FdetailApp%2Fref%3Dasc_df_02621336012890177%2F%3Fasin%3D0262133601%26tag%3Ddouban-23%26creative%3D2384%26creativeASIN%3D0262133601%26linkCode%3Ddf0&amp;srcsubj=1224802&amp;type=bkbuy&amp;subject=1776634">
                    <span class=""> 1246.00 元</span>
                  </a>
                  <p class="publish">
                    <a href="https://book.douban.com/subject/1776634/" onclick="moreurl(this,{buylink:'other'})"
                    class=" ">The MIT Press版</a>
                  </p>
                </li>
        </ul>
      </div>
      
  <div class="add2cartContainer ft no-border">
    
  <span class="add2cartWidget ll">
      <a class="j  add2cart a_show_login" href="https://www.douban.com/register?reason=addbook2cart" rel="nofollow">
        <span>+ 加入购书单</span></a>
  </span>
  

  </div>

  </div>
  <script type="text/javascript">
  $('.more-ebooks').on('click', function() {
    var $this = $(this),
      $li = $this.siblings('ul').find('li');
    if ($this.hasClass('isShow')) {
      $(this).text('展开更多').removeClass('isShow');
      $li.not(':first').addClass('hide');
    }else{
      $(this).text('收起').addClass('isShow');
      $li.removeClass('hide');
    }
    
  })
  </script>

<style class="text/css">
  .presale-indicator {
  display: inline-block;
  *display: inline;
  *zoom: 1;
  height: 15px;
  line-height: 15px;
  background: url(https://img3.doubanio.com/f/book/1679c65572eac1371f9872807199dea6e55a7f06/pics/book/cart/presale_text.gif) no-repeat;
  text-indent: -9999px;
  vertical-align: middle;
  *vertical-align: 0px;
  _vertical-align: 2px;
  margin-left: 0.5em;
}

</style>



  





<div class="gray_ad" id="borrowinfo">
  
  

  <h2>
    <span class="">在哪儿借这本书</span>
      ······

  </h2>


  <ul class="bs more-after">
      
      <li style="border: none">
        <a href="https://www.douban.com/link2/?url=http%3A%2F%2Fipac.library.sh.cn%2Fipac20%2Fipac.jsp%3Faspect%3Dbasic_search%26profile%3Dsl%26index%3DISBN%26term%3D7505399217&amp;subject=7505399217&amp;type=borrow&amp;library=10012&amp;link2key=f1c442c35f" target="_blank">上海市中心图书馆(2)</a>
      </li>
      
      <li style="border: none">
        <a href="https://www.douban.com/link2/?url=http%3A%2F%2Fourex.lib.sjtu.edu.cn%2Fprimo_library%2Flibweb%2Faction%2Fsearch.do%3Ffn%3Dsearch%26vl%28freeText0%29%3D7-5053-9921-7&amp;subject=7505399217&amp;type=borrow&amp;library=10006&amp;link2key=f1c442c35f" target="_blank">上海交通大学图书馆</a>
      </li>
      
      <li style="border: none">
        <a href="https://www.douban.com/link2/?url=http%3A%2F%2F58.154.49.3%3A8080%2Fopac%2Fopenlink.php%3FhistoryCount%3D1%26strText%3D7-5053-9921-7%26doctype%3DALL%26strSearchType%3Disbn%26match_flag%3Dforward%26displaypg%3D20%26sort%3DCATA_DATE%26orderby%3Ddesc%26showmode%3Dlist%26location%3DALL&amp;subject=7505399217&amp;type=borrow&amp;library=10010&amp;link2key=f1c442c35f" target="_blank">沈阳师范大学图书馆</a>
      </li>
  </ul>
  <div class="clearfix"></div>
  <!--<div class="ft pl">-->
    <!--<a class="rr"  href="https://book.douban.com/library_invitation">&gt; 图书馆合作</a>-->
    <!--找不到你需要的图书馆？-->
  <!--</div>-->
</div>

  <div id="dale_book_subject_top_middle"></div>
  





  

  
  

  <h2>
    <span class="">这本书的其他版本 </span>
      ······
      <span class="pl">&nbsp;(
          <a href="https://book.douban.com/works/1076262">全部2</a>
        ) </span>

  </h2>


  <div class="indent">
    <ul>
        <li class="mb8 pl">
          <a href="https://book.douban.com/subject/1776634/">The MIT Press版</a>
          1999-6-18 / 66人读过 / 有售
        </li>
    </ul>
  </div>


  



      
  

  <h2>
    <span class="">以下豆列推荐</span>
      ······
      <span class="pl">&nbsp;(
          <a href="https://book.douban.com/subject/1224802/doulists">全部</a>
        ) </span>

  </h2>


    <div id="db-doulist-section" class="indent">
      <ul class="bs">
          <li class=""><a class="" href="https://www.douban.com/doulist/105068/" target="_blank">数据挖掘&amp;信息检索相关</a>
                <span class="pl">(miner)</span>
            </li>
          <li class=""><a class="" href="https://www.douban.com/doulist/54202/" target="_blank">语义识别</a>
                <span class="pl">(烝民)</span>
            </li>
          <li class=""><a class="" href="https://www.douban.com/doulist/152314/" target="_blank">人工智能相關</a>
                <span class="pl">(dionysus)</span>
            </li>
          <li class=""><a class="" href="https://www.douban.com/doulist/1726775/" target="_blank">数据挖掘、统计、机器学习</a>
                <span class="pl">(大米粒)</span>
            </li>
          <li class=""><a class="" href="https://www.douban.com/doulist/430092/" target="_blank">数学计算机专业书籍</a>
                <span class="pl">(万籁君)</span>
            </li>
      </ul>
    </div>

  <div id="dale_book_subject_middle_mini"></div>
  






  <h2>谁读这本书?</h2>
  <div class="indent" id="collector">

    

<div class="">
    
    <div class="ll"><a href="https://www.douban.com/people/wangjunyan/"><img src="https://img3.doubanio.com/icon/u2859017-2.jpg" class="pil" alt="superfish" /></a></div>
    <div style="padding-left:60px"><a class="" href="https://www.douban.com/people/wangjunyan/">superfish</a><br/>
      <div class="pl ll">          昨天          想读      </div>

      <br/>


    </div>
    <div class="clear"></div><br/>
    <div class="ul" style="margin-bottom:12px;"></div>
</div>
<div class="">
    
    <div class="ll"><a href="https://www.douban.com/people/dylanliu3/"><img src="https://img3.doubanio.com/icon/u13412571-3.jpg" class="pil" alt="sc" /></a></div>
    <div style="padding-left:60px"><a class="" href="https://www.douban.com/people/dylanliu3/">sc</a><br/>
      <div class="pl ll">          1月11日          想读      </div>

      <br/>

      <span class="pl">tags:NLP</span><br />

    </div>
    <div class="clear"></div><br/>
    <div class="ul" style="margin-bottom:12px;"></div>
</div>
<div class="">
    
    <div class="ll"><a href="https://www.douban.com/people/41098920/"><img src="https://img1.doubanio.com/icon/u41098920-18.jpg" class="pil" alt="giber" /></a></div>
    <div style="padding-left:60px"><a class="" href="https://www.douban.com/people/41098920/">giber</a><br/>
      <div class="pl ll">          1月5日          想读      </div>

      <br/>

      <span class="pl">tags:自然语言处理 人工智能 NLP AI</span><br />

    </div>
    <div class="clear"></div><br/>
    <div class="ul" style="margin-bottom:12px;"></div>
</div>


        <p class="pl">&gt; <a href="https://book.douban.com/subject/1224802/doings">73人在读</a></p>
        <p class="pl">&gt; <a href="https://book.douban.com/subject/1224802/collections">163人读过</a></p>
        <p class="pl">&gt; <a href="https://book.douban.com/subject/1224802/wishes">616人想读</a></p>

  </div>





  
<!-- douban ad begin -->
<div id="dale_book_subject_middle_right"></div>
<script type="text/javascript">
    (function (global) {
        if(!document.getElementsByClassName) {
            document.getElementsByClassName = function(className) {
                return this.querySelectorAll("." + className);
            };
            Element.prototype.getElementsByClassName = document.getElementsByClassName;

        }
        var articles = global.document.getElementsByClassName('article'),
            asides = global.document.getElementsByClassName('aside');

        if (articles.length > 0 && asides.length > 0 && articles[0].offsetHeight >= asides[0].offsetHeight) {
            (global.DoubanAdSlots = global.DoubanAdSlots || []).push('dale_book_subject_middle_right');
        }
    })(this);
</script>
<!-- douban ad end -->

  





  

  <h2>二手市场</h2>
  <div class="indent">
    <ul class="bs">
    <li class="">
        <a class=" " href="https://book.douban.com/subject/1224802/offers">1本二手书欲转让</a>
          <span class="">
            (30.00
              元)
          </span>
      </li>
      <li>
          <a class="rr j a_show_login" href="https://www.douban.com/register?reason=secondhand-offer&amp;cat=book"><span>&gt; 点这儿转让</span></a>

        有616人想读,手里有一本闲着?
      </li>
    </ul>
  </div>

  
<p class="pl">订阅关于统计自然语言处理基础的评论: <br/><span class="feed">
    <a href="https://book.douban.com/feed/subject/1224802/reviews"> feed: rss 2.0</a></span></p>


      </div>
      <div class="extra">
        
  
<!-- douban ad begin -->
<div id="dale_book_subject_bottom_super_banner"></div>
<script type="text/javascript">
    (function (global) {
        var body = global.document.body,
            html = global.document.documentElement;

        var height = Math.max(body.scrollHeight, body.offsetHeight, html.clientHeight, html.scrollHeight, html.offsetHeight);
        if (height >= 2000) {
            (global.DoubanAdSlots = global.DoubanAdSlots || []).push('dale_book_subject_bottom_super_banner');
        }
    })(this);
</script>
<!-- douban ad end -->


      </div>
    </div>
  </div>

        
<div id="footer">

<span id="icp" class="fleft gray-link">
    &copy; 2005－2018 douban.com, all rights reserved 北京豆网科技有限公司
</span>

<a href="https://www.douban.com/hnypt/variformcyst.py" style="display: none;"></a>

<span class="fright">
    <a href="https://www.douban.com/about">关于豆瓣</a>
    · <a href="https://www.douban.com/jobs">在豆瓣工作</a>
    · <a href="https://www.douban.com/about?topic=contactus">联系我们</a>
    · <a href="https://www.douban.com/about?policy=disclaimer">免责声明</a>
    
    · <a href="https://help.douban.com/?app=book" target="_blank">帮助中心</a>
    · <a href="https://book.douban.com/library_invitation">图书馆合作</a>
    · <a href="https://www.douban.com/doubanapp/">移动应用</a>
    · <a href="https://www.douban.com/partner/">豆瓣广告</a>
</span>

</div>

    </div>
      
  

    <script type="text/javascript" src="https://img3.doubanio.com/misc/mixed_static/b93d8ec8cf50308.js"></script>
    <!-- mako -->
    
  








    
<script type="text/javascript">
    (function (global) {
        var newNode = global.document.createElement('script'),
            existingNode = global.document.getElementsByTagName('script')[0],
            adSource = '//erebor.douban.com/',
            userId = '',
            browserId = 'ngBJEephDn4',
            criteria = '7:自然语言处理|7:NLP|7:计算语言学|7:计算机|7:统计|7:人工智能|7:自然语言|7:语言学|7:算法|7:AI|3:/subject/1224802/',
            preview = '',
            debug = false,
            adSlots = ['dale_book_subject_top_icon', 'dale_book_subject_top_right', 'dale_book_subject_top_middle', 'dale_book_subject_middle_mini'];

        global.DoubanAdRequest = {src: adSource, uid: userId, bid: browserId, crtr: criteria, prv: preview, debug: debug};
        global.DoubanAdSlots = (global.DoubanAdSlots || []).concat(adSlots);

        newNode.setAttribute('type', 'text/javascript');
        newNode.setAttribute('src', 'https://img3.doubanio.com/f/adjs/d59867ddbe8a25645584b467a58a41cbd672c9be/ad.release.js');
        newNode.setAttribute('async', true);
        existingNode.parentNode.insertBefore(newNode, existingNode);
    })(this);
</script>












    
  

<script type="text/javascript">
  var _paq = _paq || [];
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var p=(('https:' == document.location.protocol) ? 'https' : 'http'), u=p+'://fundin.douban.com/';
    _paq.push(['setTrackerUrl', u+'piwik']);
    _paq.push(['setSiteId', '100001']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; 
    g.type='text/javascript';
    g.defer=true; 
    g.async=true; 
    g.src=p+'://s.doubanio.com/dae/fundin/piwik.js';
    s.parentNode.insertBefore(g,s);
  })();
</script>

<script type="text/javascript">
var setMethodWithNs = function(namespace) {
  var ns = namespace ? namespace + '.' : ''
    , fn = function(string) {
        if(!ns) {return string}
        return ns + string
      }
  return fn
}

var gaWithNamespace = function(fn, namespace) {
  var method = setMethodWithNs(namespace)
  fn.call(this, method)
}

var _gaq = _gaq || []
  , accounts = [
      { id: 'UA-7019765-1', namespace: 'douban' }
    , { id: 'UA-7019765-16', namespace: '' }
    ]
  , gaInit = function(account) {
      gaWithNamespace(function(method) {
        gaInitFn.call(this, method, account)
      }, account.namespace)
    }
  , gaInitFn = function(method, account) {
      _gaq.push([method('_setAccount'), account.id])

      
  _gaq.push([method('_addOrganic'), 'google', 'q'])
  _gaq.push([method('_addOrganic'), 'baidu', 'wd'])
  _gaq.push([method('_addOrganic'), 'soso', 'w'])
  _gaq.push([method('_addOrganic'), 'youdao', 'q'])
  _gaq.push([method('_addOrganic'), 'so.360.cn', 'q'])
  _gaq.push([method('_addOrganic'), 'sogou', 'query'])
  if (account.namespace) {
    _gaq.push([method('_addIgnoredOrganic'), '豆瓣'])
    _gaq.push([method('_addIgnoredOrganic'), 'douban'])
    _gaq.push([method('_addIgnoredOrganic'), '豆瓣网'])
    _gaq.push([method('_addIgnoredOrganic'), 'www.douban.com'])
  }

      if (account.namespace === 'douban') {
        _gaq.push([method('_setDomainName'), '.douban.com'])
      }

        _gaq.push([method('_setCustomVar'), 1, 'responsive_view_mode', 'desktop', 3])

        _gaq.push([method('_setCustomVar'), 2, 'login_status', '0', 2]);

      _gaq.push([method('_trackPageview')])
    }

for(var i = 0, l = accounts.length; i < l; i++) {
  var account = accounts[i]
  gaInit(account)
}


;(function() {
    var ga = document.createElement('script');
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    ga.setAttribute('async', 'true');
    document.documentElement.firstChild.appendChild(ga);
})()
</script>








    <!-- sindar2b-docker-->

</body>
</html>




































